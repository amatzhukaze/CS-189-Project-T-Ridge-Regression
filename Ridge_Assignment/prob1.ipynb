{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prob1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0qRTBDoGaYN_"},"source":["# 1 OLS, polynomial toy model, and motivation for Ridge"]},{"cell_type":"code","metadata":{"id":"EZ1zJ67PotYt","executionInfo":{"status":"ok","timestamp":1607655730521,"user_tz":480,"elapsed":988,"user":{"displayName":"Tejasvi Kothapalli","photoUrl":"","userId":"01989466851672073809"}}},"source":["import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline\r\n","from ipywidgets import interactive\r\n","import ipywidgets as widgets\r\n","from ipywidgets import fixed"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2WhtqT5FMY5"},"source":["### 1.1 Ordinary Least Square (OLS)"]},{"cell_type":"markdown","metadata":{"id":"zZCwKe0natXR"},"source":["Recall that for Ordinary Least Squares, our goal is to minimize the squared error of our predicitons. For each data point $\\vec{x}_i$, we try to minimize the square of the difference between our prediction, $\\hat{y_i} = \\vec{x}_i^T\\vec{w}$, and the actual value, $y_i$. This can be formulated as the following:  \\\\\n","\n","Optimization Problem: $\\underset{\\lambda}{\\text{min }} \\|y - Xw \\|_2^2$\n","\n","Closed Form Solution: $\\hat{w} = (X^TX)^{-1}X^T\\vec{y}$\n","\n","Notice here that the matrix $X$ contains $n$ rows of training points stacked on top of each other: $X = \n","\\begin{bmatrix}\n","- \\vec{x}_1^T -\\\\\n","- \\vec{x}_2^T -\\\\\n","... \\\\\n","- \\vec{x}_n^T -\n","\\end{bmatrix}$"]},{"cell_type":"markdown","metadata":{"id":"KQsWdpboFQtF"},"source":["**1.1.1. Implement the closed form solution for OLS below:**"]},{"cell_type":"code","metadata":{"id":"LmlUz_i4Ht82","executionInfo":{"status":"ok","timestamp":1607655730523,"user_tz":480,"elapsed":983,"user":{"displayName":"Tejasvi Kothapalli","photoUrl":"","userId":"01989466851672073809"}}},"source":["def ols(X, y):\n","    ### BEGIN CODE ###\n","    w_hat = ...\n","    ### END CODE ###\n","    return w_hat"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPs8xSYfFX5K"},"source":["## 1.2 Toy Model and Polynomial Regression"]},{"cell_type":"markdown","metadata":{"id":"sKKdYLMbcSeK"},"source":["Let us assume we have a polynomial function, $y=f(x)$ where $x$ and $y$ are both scalars. We want to learn a model that can predict $y$ given an $x$ using polynomial regression.\n","\n","We're given training x values of $x_1, x_2,...x_n$ and y values of $y_1, y_2,...y_n$. In the real world, collecting perfect data is impossible, and thus our y-values can be slightly off from the true function. We refer to this as **noise** in the data:\n","\n","$y_i = f(x_i) + \\delta_i$ where $\\delta_i$ is the noise for the $i^{th}$ training point. It is out of scope for now to understand what values $\\delta_i$ can take on. For now, all you need to know is that $\\delta_i$ is unpredictable and somewhat small, but you will see shortly how it can affect the training of our model."]},{"cell_type":"markdown","metadata":{"id":"4mfYU0JUFhm7"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"TzxA_7VfFfLm"},"source":["In order to perform polynomial regression, we need to lift the $x_i$ scalars into degree-$d$ polynomial features: $\\vec{x_i} = [x_i^0, x_i^1, x_i^2, ..., x_i^d]^T$\r\n","\r\n","The $X$ matrix as previously defined now consists of the $n$ feature vectors of training points stacked on top of each other: $X = \r\n","\\begin{bmatrix}\r\n","- \\vec{x}_1^T -\\\\\r\n","- \\vec{x}_2^T -\\\\\r\n","... \\\\\r\n","- \\vec{x}_n^T -\r\n","\\end{bmatrix}$"]},{"cell_type":"markdown","metadata":{"id":"C7hbq96tFccP"},"source":["**1.2.1. Run the cell below to generate and visualize our training data, and answer the questions in the next cell.**"]},{"cell_type":"code","metadata":{"id":"rwrktRKdcnyQ"},"source":["def generate_data(x_range, func, sigma=1, n=80):\n","    y_range = np.array([func(x) + np.random.normal(0, sigma) for x in x_range])\n","    random_indicies = np.arange(len(x_range))\n","    np.random.shuffle(random_indicies)\n","    x = x_range[random_indicies[:n]]\n","    y = y_range[random_indicies[:n]]\n","    return x, y\n","\n","def get_features(d, x_scalars):\n","    X = []\n","    for x in x_scalars:\n","        X.append([x**i for i in range(d+1)])\n","    return np.array(X)\n","\n","\n","# Don't overwrite these values as it was kinda hard to find a good example where\n","# ridge actually helped, just comment this out and put in a different example if\n","# you want to test on other functions/x_ranges\n","\n","x_range = np.linspace(-3, 1, 101, endpoint=True)\n","func = lambda x: x**3 + 3*x**2 - 2\n","np.random.seed(123)\n","x, y = generate_data(x_range, func, 0.4, 80)\n","\n","# Training and validation set split, we'll just use 50/50 in this case\n","N = 40\n","x_train = x[:N] \n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.plot()\n","plt.legend()\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mO9oKZYMG6X"},"source":["Read and understand the code above, and then answer these questions:"]},{"cell_type":"markdown","metadata":{"id":"muuEclQNF3_C"},"source":["**1.2.2. What is the true function we are trying to model? This is the $f(x)$ in $y_i = f(x_i) + \\delta_i$**"]},{"cell_type":"markdown","metadata":{"id":"NtcFYF1JF1dx"},"source":["ANSWER: "]},{"cell_type":"markdown","metadata":{"id":"4uWdxXXYFxRa"},"source":["**1.2.3. Say we used a degree 5 polynomial to perform polynomial regression, what is the true weight vector $\\vec{w}$ such that $f(x) = \\vec{x}^T\\vec{w}$? Recall that $\\vec{x}$ is a vector with degree 5 polynomial features $\\vec{x}=[x^0, x^1, x^2, x^3, x^4, x^5]^T$. *(hint: expand out $\\vec{x}^T\\vec{w}$ and compare it with your answer to 1.)*** "]},{"cell_type":"markdown","metadata":{"id":"xFfBfcM4FuwU"},"source":["ANSWER: "]},{"cell_type":"markdown","metadata":{"id":"ktOiG6fVFsPS"},"source":["**1.2.4. Why are the training data not on the line of the true function in the plot?**"]},{"cell_type":"markdown","metadata":{"id":"Jd0kxfbrForS"},"source":["ANSWER: "]},{"cell_type":"markdown","metadata":{"id":"DIZVJjjtF85C"},"source":["## 1.3 Overfitting of Noise using OLS"]},{"cell_type":"markdown","metadata":{"id":"2qcpwUo_6liU"},"source":["Now, we are going to learn the weights of the polynomial function by using OLS. We have 40 training points that we are going to lift into polynomial functions and use to construct the X matrix."]},{"cell_type":"markdown","metadata":{"id":"0sTpBZ2gGSTG"},"source":["**1.3.1. Fill in the code and run the cell below to visualize the learned function from Polynomial Regression using OLS.**"]},{"cell_type":"code","metadata":{"id":"vnn_mOI16rHk"},"source":["def mse(y_pred, y):\n","    return np.mean((y_pred - y)**2)\n","\n","def plot_polynomial(d):\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(x_range, func(x_range), label='True')\n","    plt.plot(x_train, y_train, 'o', label='Data')\n","\n","    X_range = get_features(d, x_range)\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","    y_pred = X_range@w\n","\n","    plt.plot(x_range, y_pred, label='Learned')\n","    plt.ylim(-5, 5)\n","    plt.legend()\n","    plt.show()\n","    print(\"Weight Vector\", w)\n","    print(\"Norm of Weight Vector:\", np.linalg.norm(w))\n","\n","ols_slider = widgets.IntSlider(value=5,\n","                               min=1,\n","                               max=10,\n","                               step=1,\n","                               description=\"Degree\")\n","interactive(plot_polynomial, d = ols_slider)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzRS5zqrJwNm"},"source":["**1.3.2. Play around with the slider to change the degree of polynomial regression. What do you notice about the function and weights when we increase the degree too much?**"]},{"cell_type":"markdown","metadata":{"id":"6QZCb9LEGfgM"},"source":["YOUR ANSWER HERE: "]},{"cell_type":"markdown","metadata":{"id":"rM9OEgFjGiAT"},"source":["## 1.4 Understanding Overfitting from Training and Test error"]},{"cell_type":"markdown","metadata":{"id":"TV0sPIdt77yS"},"source":["Why would is our model getting worse as we increase the complexity? Isn't our goal to minimize error?\n","\n","To understand this, let's see how the degree of our features affects both the training error, as well as the test error of the function. Since the test data is never used during training, the test error can give us an idea of how well the model performs on data it hasn't seen before."]},{"cell_type":"markdown","metadata":{"id":"42D5MhCYGkcP"},"source":["**1.4.1. Implement the code below to calculate the mean training error as well as mean test error *(hint: the mse method might be useful here)*. Then, run the cell to plot the training and test error over varying degrees of polynomial regression using OLS.**"]},{"cell_type":"code","metadata":{"id":"A8MrZyZP8q7t"},"source":["#Test Error vs. Degree, Train Error vs. Degree\n","train_errors, true_errors = [], []\n","\n","for degree in range(1, 11):\n","    X_test = get_features(degree, x_test)\n","    X_train = get_features(degree, x_train)\n","    ### BEGIN CODE ###\n","\n","    ### END CODE ###\n","    \n","\n","plt.figure(figsize=(10, 8))\n","plt.yscale(\"log\")\n","plt.plot(range(1, 11), train_errors, label = \"Train Error\")\n","plt.plot(range(1, 11), true_errors, label = \"Test Error\")\n","plt.legend()\n","plt.ylabel(\"Mean Squared Error\")\n","plt.xlabel(\"Degree of Polynomial\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKo0EgdCgEsa"},"source":["**1.4.2. From the plot above, what do you notice about the training and test errors as the degree increases? Does this align with the learned function you saw in the previous cell?**"]},{"cell_type":"markdown","metadata":{"id":"DqolbhNnGpAX"},"source":["YOUR ANSWER HERE: "]},{"cell_type":"markdown","metadata":{"id":"8XXDrEB3din-"},"source":["In most real world situations, we won't know details about the true underlying function. In this case, that could mean we don't know what the degree of the true polynomial function is. Let's guess that it is a degree 7 polynomial and see what happens with running Polynomial Regression with OLS."]},{"cell_type":"markdown","metadata":{"id":"tm85M7a-GsB7"},"source":["**1.4.3. Implement the code below and graph the resulting function from running Polynomial Regression with OLS with a 7 degree polynomial:**"]},{"cell_type":"code","metadata":{"id":"Icrru2aJd1XK"},"source":["plt.figure(figsize=(10, 8))\n","plt.plot(x_range, func(x_range), label='True')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","\n","D = 7\n","\n","X_range = get_features(D, x_range)\n","### BEGIN CODE ###\n","\n","### END CODE ###\n","y_pred = X_range@w\n","\n","plt.plot(x_range, y_pred, label='Learned')\n","plt.ylim(-5, 5)\n","plt.legend()\n","plt.show()\n","print(\"Weight Vector\", w)\n","print(\"Norm of Weight Vector:\", np.linalg.norm(w))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMe8X6OLgybr"},"source":["Notice that degree 7 polynomial regression with OLS doesn't perform very well given these training points. In the next section, we will introduce a slight variation of OLS called Ridge Regression that can help us fix this problem."]}]}