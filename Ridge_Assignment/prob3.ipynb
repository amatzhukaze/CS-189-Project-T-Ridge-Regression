{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prob3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"itTP87SpB15t"},"source":["# 3 Sklearn\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JLP61DzVCwMh"},"source":["**STOP: If you have not completed Problem 2, please do that first!**"]},{"cell_type":"markdown","metadata":{"id":"0_x5ASyxCvKo"},"source":["Finally, we will run through some exercises to get familiar with Sklearn, a machine learning library in python. In most of the practical settings, we won't need to write our own featurization method or ridge regression method as those are often provided for us in common libraries like Sklearn."]},{"cell_type":"code","metadata":{"id":"mcjqHdB43Yax","executionInfo":{"status":"ok","timestamp":1607656987744,"user_tz":480,"elapsed":796,"user":{"displayName":"Tejasvi Kothapalli","photoUrl":"","userId":"01989466851672073809"}}},"source":["import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dNomV_6bCUPK"},"source":["## 3.1 Sklearn on the Polynomial Toy Model dataset"]},{"cell_type":"markdown","metadata":{"id":"F2PlgWe3KzF_"},"source":["First, let's reimplement ridge regression on our polynomial toy model using the PolynomialFeatures and Ridge classes of Sklearn. We will first use sklearn's PolynomialFeatures class to lift the data into a degree 7 polynomial, then perform ridge regression on the dataset and report the training error and test error. Here's some useful documentation to get you started:\n","\n","1. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n","2. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"]},{"cell_type":"markdown","metadata":{"id":"lQe_Ehz8CYwE"},"source":["**3.1.1. Implement the code below to generate a plot of the training and testing error over a range of lambdas and report the lambda with the best test error.**"]},{"cell_type":"code","metadata":{"id":"MsCrasK1CAi1"},"source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import Ridge\n","\n","def generate_data(x_range, func, sigma=1, n=80):\n","    y_range = np.array([func(x) + np.random.normal(0, sigma) for x in x_range])\n","    random_indicies = np.arange(len(x_range))\n","    np.random.shuffle(random_indicies)\n","    x = x_range[random_indicies[:n]]\n","    y = y_range[random_indicies[:n]]\n","    return x, y\n","\n","def mse(y_pred, y):\n","    return np.mean((y_pred - y)**2)\n","\n","# Regenerating the polynomial toy model data\n","x_range = np.linspace(-3, 1, 101, endpoint=True)\n","func = lambda x: x**3 + 3*x**2 - 2\n","np.random.seed(123)\n","x, y = generate_data(x_range, func, 0.4, 80)\n","# Training and validation set split\n","N = 40\n","x_train = x[:N]\n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","D = 7\n","lambdas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n","mses_train = []\n","mses_test = []\n","best_mse = float('inf')\n","best_lambd = None\n","\n","# TODO: generate the polynomial features for X_train and X_test using PolynomialFeatures\n","### BEGIN CODE ###\n","\n","### END CODE ###\n","\n","for lambd in lambdas:\n","    # TODO: Train a Ridge model on the polynomial features and evaluate the mse for both training and test sets\n","    # HINT: the method mse() might be useful here\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","\n","plt.plot(lambdas, mses_train, label='Train')\n","plt.plot(lambdas, mses_test, label='Test')\n","plt.xscale('log')\n","plt.xlabel('log lambda')\n","plt.ylabel('MSE')\n","plt.legend()\n","plt.show()\n","\n","print(f'Best Loss: {best_mse}, Best Lambda: {best_lambd}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JE-iZ1DWCu80"},"source":["Compare this error chart with error chart we generated earlier without Sklearn. If you did the coding portion correctly, the results should be similar - which is expected."]},{"cell_type":"markdown","metadata":{"id":"R09iCL8ECb0z"},"source":["## 3.2 Sklearn on a different dataset"]},{"cell_type":"markdown","metadata":{"id":"DymY4u88BXVe"},"source":["Now that you've had an introduction to the Ridge and PolynomialFeature classes of the Sklearn library, let's practice implementing ridge regression using Sklearn on a different dataset."]},{"cell_type":"markdown","metadata":{"id":"w-b-KpJXCgZt"},"source":["**3.2.1 Run the cell below to visualize our new dataset:**"]},{"cell_type":"code","metadata":{"id":"--2eu16B5-j2"},"source":["x_range = np.linspace(-1, 1, 101, endpoint=True)\n","func = lambda x: np.exp(x)\n","np.random.seed(12)\n","x, y = generate_data(x_range, func, .4, 80)\n","\n","# Training and validation set split, we'll just use 50/50 in this case\n","N = 40\n","x_train = x[:N]\n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","# Plot the true function and training data\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.plot()\n","plt.legend()\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMsNHuqQDzwI"},"source":["Again, let's try to fit this data with a degree 7 polynomial with ridge regression.\n","\n","**3.2.2. Using the Sklearn library, write code below to visualize the training and testing errors over a range of lambdas and report the best lambda.*(hint: feel free to reference 3.1.1 to get you started)***"]},{"cell_type":"code","metadata":{"id":"_Cj3IMwm_Nfo"},"source":["### BEGIN CODE ###\n","\n","### END CODE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iA15xYpgD7P9"},"source":["**3.2.3. Fill in the best lambda below and run the cell to visualize our learned function. Do you think this was a good choice of lambda? Try other values of lambda as well and report your observations.**"]},{"cell_type":"markdown","metadata":{"id":"J8k3_4kDClTd"},"source":["YOUR OBSERVATIONS HERE: "]},{"cell_type":"code","metadata":{"id":"49VBVDWl_iTg"},"source":["LAMBD = 1 # Edit this\n","sklearn_ridge = Ridge(alpha=LAMBD)\n","sklearn_ridge.fit(X_train, y_train)\n","X_range = poly.fit_transform(np.array(x_range).reshape(len(x_range),1))\n","y_range_pred = sklearn_ridge.predict(X_range)\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_range, y_range_pred, label=f'Predicted Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vpLLGLr5CofN"},"source":["## 3.3 Additional Datasets for Practice"]},{"cell_type":"markdown","metadata":{"id":"Q1saLgyZGgfH"},"source":["**3.3.1. Here are some more datasets you can use to practice implementing Ridge Regression with the Sklearn library. Feel free to explore other degrees of polynomial features as well.**\n"]},{"cell_type":"code","metadata":{"id":"3iZ11KJ7At0A"},"source":["dataset = 'sine' # Dataset 1: Sine Function\n","# dataset = 'sqrt' # Dataset 2: Square Root Function\n","# dataset = 'inverse' # Dataset 3: Inverse Function\n","\n","if dataset == 'sine':\n","    x_range = np.linspace(-2, 2, 101, endpoint=True)\n","    func = lambda x: np.sin(x)\n","    np.random.seed(12)\n","    x, y = generate_data(x_range, func, .4, 80)\n","\n","if dataset == 'sqrt':\n","    x_range = np.linspace(0, 10, 101, endpoint=True)\n","    func = lambda x: np.sqrt(x)\n","    np.random.seed(12)\n","    x, y = generate_data(x_range, func, .4, 80)\n","\n","if dataset == 'inverse':\n","    x_range = np.linspace(.5, 1.5, 101, endpoint=True)\n","    func = lambda x: 1/x\n","    np.random.seed(12)\n","    x, y = generate_data(x_range, func, .2, 80)\n","    \n","# Training and validation set split, using 50/50 for now\n","N = 40\n","x_train = x[:N]\n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","# Plot the true function and training data\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.plot()\n","plt.legend()\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[]}]}