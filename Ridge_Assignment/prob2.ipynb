{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prob2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"W8pINZc1JBRK"},"source":["# 2 Ridge Regression"]},{"cell_type":"markdown","metadata":{"id":"ibQVnLDN0EbD"},"source":["**STOP: If you have not completed Problem 1, please do that first!**"]},{"cell_type":"code","metadata":{"id":"zQgv7tLA69P_","executionInfo":{"status":"ok","timestamp":1607656261831,"user_tz":480,"elapsed":975,"user":{"displayName":"Tejasvi Kothapalli","photoUrl":"","userId":"01989466851672073809"}}},"source":["import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline\r\n","from ipywidgets import interactive\r\n","import ipywidgets as widgets\r\n","from ipywidgets import fixed\r\n","from tqdm import tqdm"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbOC3PNi7hH5"},"source":["We will be using data and functions from the previous problem."]},{"cell_type":"code","metadata":{"id":"rwrktRKdcnyQ"},"source":["# Functions that we will need\n","def generate_data(x_range, func, sigma=1, n=80):\n","    y_range = np.array([func(x) + np.random.normal(0, sigma) for x in x_range])\n","    random_indicies = np.arange(len(x_range))\n","    np.random.shuffle(random_indicies)\n","    x = x_range[random_indicies[:n]]\n","    y = y_range[random_indicies[:n]]\n","    return x, y\n","\n","def get_features(d, x_scalars):\n","    X = []\n","    for x in x_scalars:\n","        X.append([x**i for i in range(d+1)])\n","    return np.array(X)\n","\n","def mse(y_pred, y):\n","    return np.mean((y_pred - y)**2)\n","\n","# Datapoints\n","x_range = np.linspace(-3, 1, 101, endpoint=True)\n","func = lambda x: x**3 + 3*x**2 - 2\n","np.random.seed(123)\n","x, y = generate_data(x_range, func, 0.4, 80)\n","\n","# Training and validation set split, we'll just use 50/50 in this case\n","N = 40\n","x_train = x[:N] \n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.plot()\n","plt.legend()\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eOoMc-pX1DNZ"},"source":["## 2.1 Closed Form Solution"]},{"cell_type":"markdown","metadata":{"id":"fU5fHghFC7QI"},"source":["We've seen how when our data is noisy, making our model more complex can actually make our predictions worse. So how can we account for this when we don't know the true model? One strategy is to control the complexity of the model by adding a penalty for weight vectors with higher magnitudes. Let us add a term to the OLS optimization problem that will penalize the weights if they get too large.\n","\n","Optimization Problem: $\\underset{\\lambda}{\\text{min }} \\|y - Xw \\|_2^2 + \\lambda \\|w\\|_2^2$\n","\n","Closed Form Solution: $\\hat{w} = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$\n","\n","We refer to this method as **Ridge Regression**. "]},{"cell_type":"markdown","metadata":{"id":"o6u6Ua1w04nW"},"source":["**2.1.1. One way we can derive the closed form solution for Ridge Regression is by setting the derivative of the optimization problem to 0 and solving for $w$. Given that the derivative of $\\underset{\\lambda}{\\text{min }} \\|y - Xw \\|_2^2 + \\lambda \\|w\\|_2^2$ with respect to $w$ is:**\r\n","\r\n","  $$-2X^Ty+2X^TXw+2\\lambda w$$\r\n","\r\n","  **Show the derivation for the closed form solution above.**"]},{"cell_type":"markdown","metadata":{"id":"igI-sx000XbM"},"source":["YOUR ANSWER HERE: "]},{"cell_type":"markdown","metadata":{"id":"zfo70bfn0Sfq"},"source":["Now let us try to fit the same training data as before with a degree 7 polynomial, except this time we will use Ridge Regression instead of OLS.\r\n","\r\n","As an additional note, we suggest you use sklearn to confirm that your ridge implementation was done correctly. The ridge implementation for the sklearn is similar to the OLS version except now you have to pass in a lambda parameter when creating the model. You can use the sklearn method to make sure that your `w_ridge` and the sklearn ridge regression output the same weights. However you are NOT allowed to use any sklearn in the `ridge` function itself. Simply use sklearn as a tool to ensure that your implementation was right. Use the following link for documention of the sklearn method:\r\n","\r\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"]},{"cell_type":"markdown","metadata":{"id":"m8p-Nh9l0OuC"},"source":["**2.1.2. Fill in the code below to implement ridge regression using *only* numpy. We have also implemented the sklearn version of ridge regression in `ridge_sklearn`, use this function to verify your solution for ridge regression. Run the cell to see the learned function using ridge regression.**"]},{"cell_type":"code","metadata":{"id":"6TLm4E5s8wt5"},"source":["from sklearn.linear_model import Ridge\n","def ridge(X, y, lambd=1):\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","\n","def ridge_sklearn(X, y, lambd=1):\n","    # The alpha parameter is the same as lambda, \n","    # fit_intercept is false because we dont want sklearn to introduce any new features\n","    sklearn_model = Ridge(alpha = lambd, fit_intercept = False) \n","    sklearn_model.fit(X, y)\n","    return sklearn_model.coef_\n","\n","def plot_ridge(LAMBDA, D):\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(x_range, func(x_range), label='True')\n","    plt.plot(x_train, y_train, 'o', label='Data')\n","\n","    X_range = get_features(D, x_range)\n","    X_train = get_features(D, x_train)\n","    X_test = get_features(D, x_test)\n","\n","    ### BEGIN CODE ###\n","    # TODO: find the solution using the ridge function\n","    \n","    ### END CODE ###\n","\n","    ### BEGIN CODE ###\n","    # TODO: Find the solution using the ridge_sklearn function\n","    # TODO: Verify that the two functions return the same solution\n","    # HINT: Use np.isclose, due to the nature of floating point the two functions\n","    # may have a very small difference in output\n","    \n","    ### END CODE ###\n","\n","    y_pred_ridge = X_range@w_ridge\n","    y_pred = X_train@w_ridge\n","    y_pred_test = X_test@w_ridge\n","    plt.plot(x_range, y_pred_ridge, label=f'lam:{LAMBDA}')\n","    plt.ylim(-5, 5)\n","    plt.legend()\n","    plt.show()\n","    print(\"Weight Vector\", w_ridge)\n","    print(\"Magnitude of Weight Vector:\", np.linalg.norm(w_ridge))\n","    print(\"Train Error:\", mse(y_pred, y_train))\n","    print(\"Test Error:\", mse(y_pred_test, y_test))\n","\n","ridge_slider = widgets.FloatLogSlider(value = 0.1, base = 10, min = -3, max = 3, step = 0.5, description = \"lambda\")\n","interactive(plot_ridge, LAMBDA = ridge_slider, D = fixed(7))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G0oHTrf27L60"},"source":["**2.1.3 As we increase $\\lambda$, the shape of our function changes. How do the following depend on $\\lambda$?**\n","  *   Complexity (aka magnitude of the weight vector)\n","  *   Train Error\n","  *   Test Error"]},{"cell_type":"markdown","metadata":{"id":"XhfMP6L41jcZ"},"source":["YOUR ANSWER HERE: "]},{"cell_type":"markdown","metadata":{"id":"RGCmlHzm1fhU"},"source":["**2.1.4 Which one do we think is most important to optimize? What $\\lambda$ should we choose?**"]},{"cell_type":"markdown","metadata":{"id":"XrIuig841dSp"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"6RqeB0AH1akb"},"source":["**2.1.5 What would happen if $\\lambda$ goes to 0? (hint: consider the OLS closed form solution)**"]},{"cell_type":"markdown","metadata":{"id":"MCNGf_8k1Veu"},"source":["YOUR ANSWER HERE: "]},{"cell_type":"markdown","metadata":{"id":"7Z1U9C3J1SqJ"},"source":["**2.1.6. What happens to $w$ if $\\lambda$ goes to infinity?**"]},{"cell_type":"markdown","metadata":{"id":"Dxi9dx351MFP"},"source":["YOUR ANSWER HERE: "]},{"cell_type":"markdown","metadata":{"id":"E1O-aJau1ue8"},"source":["## 2.2 Choosing Lambda"]},{"cell_type":"markdown","metadata":{"id":"UYdBH6WkUfLt"},"source":["In Ridge Regression, $\\lambda$ is inherent to our model, and thus, must be picked by the person creating the model. We refer to this as a **hyperparameter**. What $\\lambda$ works best depends on the data itself. The higher $\\lambda$ is, the more it penalizes complexity, and thus regularizes the output.\n","\n","In this case, we'll show which $\\lambda$ works best by testing against our true function. However, in the real world, where we don't know the true model, it's best to choose hyperparameters through cross-validation."]},{"cell_type":"markdown","metadata":{"id":"pd4roPIs1qXW"},"source":["**2.2.1. Run the cell next cell and report the best $\\lambda$.**"]},{"cell_type":"code","metadata":{"id":"mbrP5umf9yR9"},"source":["mses = []\n","lambdas_mse = np.logspace(-5, 5, 10)\n","\n","best_loss = float('inf')\n","best_lambd_mse = None\n","X_test = get_features(7, x_test)\n","X_train = get_features(7, x_train)\n","\n","for lambd in lambdas_mse:\n","    w = ridge(X_train, y_train, lambd)\n","    y_pred_test = X_test@w\n","    loss = mse(y_pred_test, y_test)\n","    mses.append(loss)\n","    if best_loss > loss:\n","        best_loss = loss\n","        best_lambd_mse = lambd\n","\n","plt.plot(lambdas_mse, mses)\n","plt.xscale('log')\n","plt.xlabel('log lambda')\n","plt.ylabel('Testing MSE')\n","plt.show()\n","\n","print(f'Best Loss: {best_loss}, Best Lambda: {best_lambd_mse}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95W2hVnDXLDn"},"source":["## 2.3 Bias Variance Trade Off"]},{"cell_type":"markdown","metadata":{"id":"UgFDrrSbUDJZ"},"source":["In order to further understand why we must tune $\\lambda$, let's look at a very important concept in machine learning: Bias-Variance Trade Off. Please note that the mathematical expressions in this toy example have been simplified to help with understanding and convey the big ideas.  In order to understand this concept, let's start by observing two probability based expressions: $E[|\\hat{w}- w|_2]$ and $Var[|\\hat{w}|_2]$.\n","\n","1.   $E[|\\hat{w}- w|_2]$: This term is the *'bias'* of our model. At a high-level, this term represents the expected distance between our predicted model weights and the true model weights. $|\\cdot|_2$ refers to the *euclidean norm*. $\\hat{w}$ is the predicted weights from ridge regression. $w$ is the true underlying model's weights. Since we are dealing with vectors, $|\\hat{w}- w|_2$ is essentially a 'distance' measure for how far $\\hat{w}$ is from $w$. The $E[\\cdot]$, here, refers to *expectation* of the enclosed quantity. However, the concept of expectation does not need to be understood for this example case. We will instead use *mean* as a proxy for expectation. (Note: The *real* bias term represents how far the predicted $y$ is from the real $y$. However, the math required in analyzing this would be a little more sticky and therefore *our toy* bias term compares the predicted weight with the real weight.)\n","\n","2.   $Var[|\\hat{w}|_2]$: This term is the *'variance'* of our model. This term can be likened to the spread of the predicted model weight. The $Var[\\cdot]$ refers to the variance of the enclosed quantity. Like expectation, the concept of variance is not needed in this example. Instead, we will use the square of the *standard deviation*. (Note: The *real* variance term measures the spread of the predicted $y$. However, our toy variance term just measures the spread of the predicted weights.)"]},{"cell_type":"markdown","metadata":{"id":"scPqwYpT30MO"},"source":["Why are these bias and variance expressions important? Let's answer some questions about it to better understand: \r\n","\r\n","**2.3.1. Intuitively we want both terms to be as close to 0 as possible (both terms are bounded by 0). Let's think about bias first. Why do we want bias to be close to 0?**"]},{"cell_type":"markdown","metadata":{"id":"iINFyQayEVgM"},"source":["YOUR ANSWER HERE: "]},{"cell_type":"markdown","metadata":{"id":"ypRAEPxWEQJL"},"source":["**2.3.2. Next, lets think about variance. Why should variance be close to 0?**"]},{"cell_type":"markdown","metadata":{"id":"BNr7IrN9EOLz"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"MS21vllaEcpC"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"wSqg02kC3wtJ"},"source":["Now, we understand that we must tune $\\lambda$ in a way that minimizes both $E[|\\hat{w}- w|_2]$ and $Var[|\\hat{w}|_2]$. However, how do we calculate these two quantities? First we fix $\\lambda$ and then repeatedly draw a dataset $(X_i, \\vec{y}_i)$ and compute the associated $\\hat{w}_i$, $r$ times.\r\n","\r\n","\r\n","$$\\hat{w}_i = (X_i^TX_i + \\lambda I)^{-1}X_i^T\\vec{y}_i\\\\ \r\n","(X_i, \\vec{y}_i), \\quad 1 \\leq i \\leq r\r\n","$$\r\n","\r\n","Then compute the mean and standard deviation associated to the fixed $\\lambda$:\r\n","$$ E[|\\hat{w}(\\lambda)- w|_2] \\approx \\text{mean}(|\\hat{w}_i- w|_2) = \\frac{1}{r} \\sum_{i = 1}^r |\\hat{w}_i- w|_2\\\\\r\n","Var[|\\hat{w}(\\lambda)|_2] \\approx [\\text{standard deviation}(|\\hat{w}_i|_2)]^2 = \\frac{1}{r} \\sum_{i = 1}^r \\lvert |\\hat{w}_i|_2 - \\mu \\rvert^2, \\quad\r\n","\\mu = \\frac{1}{r} \\sum_{i = 1}^r |\\hat{w}_i|_2\r\n","$$\r\n","\r\n","This procedure is repeated over a range of $\\lambda$'s. Note that $w$ is a known value in our experiment because we know what the true model is. However, this is often not the case in machine learning.\r\n","\r\n","Now, lets observe how $E[|\\hat{w}(\\lambda)- w|_2]$ and $Var[|\\hat{w}(\\lambda)|_2]$ vary with $\\lambda$."]},{"cell_type":"markdown","metadata":{"id":"OPe3IVQpEtQG"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"ncbMBQ0TKziB"},"source":["**3.3. Fill in the following code to compute the bias and variance for each $\\lambda$**"]},{"cell_type":"code","metadata":{"id":"bSv7R1KGGuVb"},"source":["def get_train_and_test_data(x_range, degree, function, train_test_split):\n","    X, Y = generate_data(x_range, function, 1, 80)\n","\n","    indicies = np.arange(len(X))\n","    np.random.shuffle(indicies)\n","    split_index = int(train_test_split * len(X))\n","    train_indicies = indicies[:split_index]\n","    test_indicies = indicies[split_index:]\n","\n","\n","    X_train = get_features(degree, X[train_indicies])\n","    Y_train = Y[train_indicies]\n","\n","    X_test = get_features(degree, X[test_indicies])\n","    Y_test = Y[test_indicies]\n","\n","    return X_train, Y_train, X_test, Y_test\n","\n","lambdas_bv = np.logspace(-5, 5, 50)\n","\n","function = lambda x: x**3 + 3*x**2 - 2\n","w_true = np.array([-2, 0, 3, 1, 0, 0, 0, 0])\n","repeat = 600\n","biases = []\n","variances = []\n","X = np.linspace(-3, 1, 101, endpoint=True)\n","np.random.seed(46545645)\n","\n","for lam in tqdm(lambdas_bv):\n","    w_hats_norm = []\n","    distance = []\n","    for _ in range(repeat):\n","        X_train, Y_train, _, _ = get_train_and_test_data(X, 7, function, 0.8)\n","\n","        # TODO: Compute w_hat and then append appropriate values to w_hats_norm and distance\n","        ### BEGIN CODE ###\n","        \n","        ### END CODE ###\n","\n","    # TODO: Take the mean and standard devaiation squared of the distance and w_hats_norm array, respectively\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","\n","biases =  np.array(biases)\n","variances = np.array(variances)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MoJQxe5_vr73"},"source":["**2.3.4. Plot bias and variance as lambda varies.**\n","\n","Make sure to use a log scale for the lambda axis."]},{"cell_type":"code","metadata":{"id":"_LQx5S0eOM5G"},"source":["### BEGIN CODE ###\n","\n","### END CODE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naapQ6D7d0Ak"},"source":["**2.3.5. How does *bias* change as $\\lambda$ increases? Why does this make intuitive sense (Hint: think about how a larger $\\lambda$ will force the magnitude of $\\hat{w}$ to be smaller)?** "]},{"cell_type":"markdown","metadata":{"id":"J9D4c_YMt9my"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"6js3E9Q0t9f3"},"source":["**2.3.6. How does *variance* change as lambda increases? Why does this make intuitive sense(Hint: similar to the previous question, think about how a larger $\\lambda$ will force the magnitude of $\\hat{w}$ to be smaller)**"]},{"cell_type":"markdown","metadata":{"id":"h3HM7Qkqt9Y7"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"otf7qN5ot9SK"},"source":["**2.3.7. What is the significance of the word *trade off* in bias variance trade off? (Hint: think about how variance and bias vary as lambda increases)**"]},{"cell_type":"markdown","metadata":{"id":"PsxBgd5lt9KP"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"n8pu2vU9LIwD"},"source":["As stated above, we want both bias and variance to be close to zero. Therefore our objective is to minimize the combination of bias and variance. The exact objective function we want to minimize is $E[|\\hat{w}- w|_2]^2 + Var[|\\hat{w}|_2]$. The reason why the bias term is squared is beyond the scope of this notebook. The fancy way to write this notion in math is shown below:\n","\n","$$\\underset{\\lambda}{\\text{min }} E[|\\hat{w}- w|_2]^2 + Var[|\\hat{w}|_2]\\\\\n","\\underset{\\lambda}{\\text{min }} \\text{bias}^2 + \\text{variance}\n","$$\n","\n","Notice here that the *optimization variable* (the variable which we vary to try to reduce the objective function) is $\\lambda$. This is because the only parameter that we can change in ridge regression is the $\\lambda$ (aside from how we featurize the data).\n"]},{"cell_type":"markdown","metadata":{"id":"tyWwbkf5LOeF"},"source":["**2.3.8. Plot how $\\text{bias}^2 + \\text{variance}$ varies with $\\lambda$. Also plot MSE vs $\\lambda$ graph from the previous section.**\n","\n","The code will also compute the optimal $\\lambda$ based on where the lowest $\\text{bias}^2 + \\text{variance}$ is achieved. "]},{"cell_type":"code","metadata":{"id":"PBkIXFYlLb0F"},"source":["### BEGIN CODE ###\n","\n","### END CODE ###\n","\n","ymin = np.min(bias_squared_plus_variance)\n","xpos = list(bias_squared_plus_variance).index(ymin)\n","best_lambd_bv = lambdas_bv[xpos]\n","\n","print(f'Best Lambda (according to bias^2 + variance): {best_lambd_bv} ')\n","print(f'Best Lambda (according to MSE): {best_lambd_mse} ')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZU0PF3YLkis"},"source":["**2.3.9. How does the best lambda according to the $\\text{bias}^2 + \\text{variance}$ compare to the best lambda according to the MSE? (Hint: think about where the 'dip' occurs in both plots)**"]},{"cell_type":"markdown","metadata":{"id":"GndwiPD1uT-i"},"source":["YOUR ANSWER HERE:"]}]}