{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ridge Assignment.ipynb","provenance":[{"file_id":"1zADvLlTE-yUv9yzxJxgiT_UGA9lOFL0o","timestamp":1606797005114}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"2QD7-xKoHhpO"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from ipywidgets import interactive\n","import ipywidgets as widgets\n","from ipywidgets import fixed\n","from tqdm import tqdm\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0qRTBDoGaYN_"},"source":["# 1 OLS, polynomial toy model, and motivation for Ridge"]},{"cell_type":"markdown","metadata":{"id":"zZCwKe0natXR"},"source":["### 1.1 Ordinary Least Square (OLS)\n","Recall that for Ordinary Least Squares, our goal is to minimize the squared error of our predicitons. For each data point $\\vec{x}_i$, we try to minimize the square of the difference between our prediction, $\\hat{y_i} = \\vec{x}_i^T\\vec{w}$, and the actual value, $y_i$. This can be formulated as the following:  \\\\\n","\n","Optimization Problem: $\\underset{\\lambda}{\\text{min }} \\|y - Xw \\|_2^2$\n","\n","Closed Form Solution: $\\hat{w} = (X^TX)^{-1}X^T\\vec{y}$\n","\n","Notice here that the matrix $X$ contains $n$ rows of training points stacked on top of each other: $X = \n","\\begin{bmatrix}\n","- \\vec{x}_1^T -\\\\\n","- \\vec{x}_2^T -\\\\\n","... \\\\\n","- \\vec{x}_n^T -\n","\\end{bmatrix}$\n","\n","\n","**1.1.1. Implement the closed form solution for OLS below:**"]},{"cell_type":"code","metadata":{"id":"LmlUz_i4Ht82"},"source":["def ols(X, y):\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","    return w_hat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKKdYLMbcSeK"},"source":["## 1.2 Toy Model and Polynomial Regression\n","Let us assume we have a polynomial function, $y=f(x)$ where $x$ and $y$ are both scalars. We want to learn a model that can predict $y$ given an $x$ using polynomial regression.\n","\n","We're given training x values of $x_1, x_2,...x_n$ and y values of $y_1, y_2,...y_n$. In the real world, collecting perfect data is impossible, and thus our y-values can be slightly off from the true function. We refer to this as **noise** in the data:\n","\n","$y_i = f(x_i) + \\delta_i$ where $\\delta_i$ is the noise for the $i^{th}$ training point. It is out of scope for now to understand what values $\\delta_i$ can take on. For now, all you need to know is that $\\delta_i$ is unpredictable and somewhat small, but you will see shortly how it can affect the training of our model.\n","\n","---\n","\n","In order to perform polynomial regression, we need to lift the $x_i$ scalars into degree-$d$ polynomial features: $\\vec{x_i} = [x_i^0, x_i^1, x_i^2, ..., x_i^d]^T$\n","\n","The $X$ matrix as previously defined now consists of the $n$ feature vectors of training points stacked on top of each other: $X = \n","\\begin{bmatrix}\n","- \\vec{x_1}^T -\\\\\n","- \\vec{x_2}^T -\\\\\n","... \\\\\n","- \\vec{x_n}^T -\n","\\end{bmatrix}$\n","\n","**1.2.1. Run the cell below to generate and visualize our training data, and answer the questions in the next cell.**\n"]},{"cell_type":"code","metadata":{"id":"rwrktRKdcnyQ"},"source":["def generate_data(x_range, func, sigma=1, n=80):\n","    y_range = np.array([func(x) + np.random.normal(0, sigma) for x in x_range])\n","    random_indicies = np.arange(len(x_range))\n","    np.random.shuffle(random_indicies)\n","    x = x_range[random_indicies[:n]]\n","    y = y_range[random_indicies[:n]]\n","    return x, y\n","\n","def get_features(d, x_scalars):\n","    X = []\n","    for x in x_scalars:\n","        X.append([x**i for i in range(d+1)])\n","    return np.array(X)\n","\n","\n","# Don't overwrite these values as it was kinda hard to find a good example where\n","# ridge actually helped, just comment this out and put in a different example if\n","# you want to test on other functions/x_ranges\n","\n","x_range = np.linspace(-3, 1, 101, endpoint=True)\n","func = lambda x: x**3 + 3*x**2 - 2\n","np.random.seed(123)\n","x, y = generate_data(x_range, func, 0.4, 80)\n","\n","# Training and validation set split, we'll just use 50/50 in this case\n","N = 40\n","x_train = x[:N] \n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.plot()\n","plt.legend()\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mO9oKZYMG6X"},"source":["Read and understand the code above, and then answer these questions:\n","\n","**1.2.2. What is the true function we are trying to model? This is the $f(x)$ in $y_i = f(x_i) + \\delta_i$**\n","\n","YOUR ANSWER HERE:\n","\n","**1.2.3. Say we used a degree 5 polynomial to perform polynomial regression, what is the true weight vector $\\vec{w}$ such that $f(x) = \\vec{x}^T\\vec{w}$? Recall that $\\vec{x}$ is a vector with degree 5 polynomial features $\\vec{x}=[x^0, x^1, x^2, x^3, x^4, x^5]^T$. *(hint: expand out $\\vec{x}^T\\vec{w}$ and compare it with your answer to 1.)*** \n","\n","YOUR ANSWER HERE:\n","\n","**1.2.4. Why are the training data not on the line of the true function in the plot?**\n","\n","YOUR ANSWER HERE:\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2qcpwUo_6liU"},"source":["## 1.3 Overfitting of Noise using OLS\n","Now, we are going to learn the weights of the polynomial function by using OLS. We have 40 training points that we are going to lift into polynomial functions and use to construct the X matrix.\n","\n","talk about how we lift from scalars\n","\n","**1.3.1. Fill in the code and run the cell below to visualize the learned function from Polynomial Regression using OLS.**"]},{"cell_type":"code","metadata":{"id":"vnn_mOI16rHk"},"source":["def mse(y_pred, y):\n","    return np.mean((y_pred - y)**2)\n","\n","def plot_polynomial(d):\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(x_range, func(x_range), label='True')\n","    plt.plot(x_train, y_train, 'o', label='Data')\n","\n","    X_range = get_features(d, x_range)\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","    y_pred = X_range@w\n","\n","    plt.plot(x_range, y_pred, label='Learned')\n","    plt.ylim(-5, 5)\n","    plt.legend()\n","    plt.show()\n","    print(\"Weight Vector\", w)\n","    print(\"Norm of Weight Vector:\", np.linalg.norm(w))\n","\n","ols_slider = widgets.IntSlider(value=5,\n","                               min=1,\n","                               max=10,\n","                               step=1,\n","                               description=\"Degree\")\n","interactive(plot_polynomial, d = ols_slider)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzRS5zqrJwNm"},"source":["**1.3.2. Play around with the slider to change the degree of polynomial regression. What do you notice about the function and weights when we increase the degree too much?**\n","\n","YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"TV0sPIdt77yS"},"source":["## 1.4 Understanding Overfitting from Training and Test error\n","\n","Why would is our model getting worse as we increase the complexity? Isn't our goal to minimize error?\n","\n","To understand this, let's see how the degree of our features affects both the training error, as well as the test error of the function. Since the test data is never used during training, the test error can give us an idea of how well the model performs on data it hasn't seen before.\n","\n","**1.4.1. Implement the code below to calculate the mean training error as well as mean test error *(hint: the mse method might be useful here)*. Then, run the cell to plot the training and test error over varying degrees of polynomial regression using OLS.**"]},{"cell_type":"code","metadata":{"id":"A8MrZyZP8q7t"},"source":["#Test Error vs. Degree, Train Error vs. Degree\n","train_errors, true_errors = [], []\n","\n","for degree in range(1, 11):\n","    X_test = get_features(degree, x_test)\n","    X_train = get_features(degree, x_train)\n","    w = ols(X_train, y_train)\n","    y_pred = X_test@w\n","    y_pred_data = X_train@w\n","\n","    train_errors.append(mse(y_pred_data, y_train))\n","    true_errors.append(mse(y_pred, y_test))\n","\n","plt.figure(figsize=(10, 8))\n","plt.yscale(\"log\")\n","plt.plot(range(1, 11), train_errors, label = \"Train Error\")\n","plt.plot(range(1, 11), true_errors, label = \"Test Error\")\n","plt.legend()\n","plt.ylabel(\"Mean Squared Error\")\n","plt.xlabel(\"Degree of Polynomial\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKo0EgdCgEsa"},"source":["**1.4.2. From the plot above, what do you notice about the training and test errors as the degree increases? Does this align with the learned function you saw in the previous cell?**\n","\n","  YOU ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"8XXDrEB3din-"},"source":["In most real world situations, we won't know details about the true underlying function. In this case, that could mean we don't know what the degree of the true polynomial function is. Let's guess that it is a degree 7 polynomial and see what happens with running Polynomial Regression with OLS.\n","\n","**1.4.3. Implement the code below and graph the resulting function from running Polynomial Regression with OLS with a 7 degree polynomial:**"]},{"cell_type":"code","metadata":{"id":"Icrru2aJd1XK"},"source":["plt.figure(figsize=(10, 8))\n","plt.plot(x_range, func(x_range), label='True')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","\n","D = 7\n","\n","X_range = get_features(D, x_range)\n","### BEGIN CODE ###\n","\n","### END CODE ###\n","y_pred = X_range@w\n","\n","plt.plot(x_range, y_pred, label='Learned')\n","plt.ylim(-5, 5)\n","plt.legend()\n","plt.show()\n","print(\"Weight Vector\", w)\n","print(\"Norm of Weight Vector:\", np.linalg.norm(w))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMe8X6OLgybr"},"source":["Notice that degree 7 polynomial regression with OLS doesn't perform very well given these training points. In the next section, we will introduce a slight variation of OLS called Ridge Regression that can help us fix this problem."]},{"cell_type":"markdown","metadata":{"id":"W8pINZc1JBRK"},"source":["# 2 Ridge Regression"]},{"cell_type":"markdown","metadata":{"id":"fU5fHghFC7QI"},"source":["## 2.1 Closed Form Solution\n","\n","We've seen how when our data is noisy, making our model more complex can actually make our predictions worse. So how can we account for this when we don't know the true model? One strategy is to control the complexity of the model by adding a penalty for weight vectors with higher magnitudes. Let us add a term to the OLS optimization problem that will penalize the weights if they get too large.\n","\n","Optimization Problem: $\\underset{\\lambda}{\\text{min }} \\|y - Xw \\|_2^2 + \\lambda \\|w\\|_2^2$\n","\n","Closed Form Solution: $\\hat{w} = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$\n","\n","We refer to this method as **Ridge Regression**. \n","\n","**2.1.1. One way we can derive the closed form solution for Ridge Regression is by setting the derivative of the optimization problem to 0 and solving for $w$. Given that the derivative of $\\underset{\\lambda}{\\text{min }} \\|y - Xw \\|_2^2 + \\lambda \\|w\\|_2^2$ with respect to $w$ is:**\n","\n","  $-2X^Ty+2X^TXw+2\\lambda w$\n","\n","  **Show the derivation for the closed form solution above.**\n","\n","  YOUR ANSWER HERE: \n","\n","\n","\n","\n","\n","Now let us try to fit the same training data as before with a degree 7 polynomial, except this time we will use Ridge Regression instead of OLS.\n","\n","**2.1.2. Fill in the code below to implement ridge regression and run the cell to see the learned function using ridge regression.**"]},{"cell_type":"code","metadata":{"id":"6TLm4E5s8wt5"},"source":["def ridge(X, y, lambd=1):\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","\n","\n","def plot_ridge(LAMBDA, D):\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(x_range, func(x_range), label='True')\n","    plt.plot(x_train, y_train, 'o', label='Data')\n","\n","    X_range = get_features(D, x_range)\n","    X_train = get_features(D, x_train)\n","    X_test = get_features(D, x_test)\n","\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","\n","    y_pred_ridge = X_range@w_ridge\n","    y_pred = X_train@w_ridge\n","    y_pred_test = X_test@w_ridge\n","    plt.plot(x_range, y_pred_ridge, label=f'lam:{LAMBDA}')\n","    plt.ylim(-5, 5)\n","    plt.legend()\n","    plt.show()\n","    print(\"Weight Vector\", w_ridge)\n","    print(\"Magnitude of Weight Vector:\", np.linalg.norm(w_ridge))\n","    print(\"Train Error:\", mse(y_pred, y_train))\n","    print(\"Test Error:\", mse(y_pred_test, y_test))\n","\n","ridge_slider = widgets.FloatLogSlider(value = 0.1, base = 10, min = -3, max = 3, step = 0.5, description = \"lambda\")\n","interactive(plot_ridge, LAMBDA = ridge_slider, D = fixed(7))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G0oHTrf27L60"},"source":["**2.1.3 As we increase $\\lambda$, the shape of our function changes. How do the following depend on $\\lambda$?**\n","  *   Complexity (aka magnitude of the weight vector)\n","  *   Train Error\n","  *   Test Error\n","\n","YOUR ANSWER HERE: \n","\n","**2.1.4 Which one do we think is most important to optimize? What $\\lambda$ should we choose?**\n","\n","  YOUR ANSWER HERE: \n","\n","**2.1.5 What would happen if $\\lambda$ goes to 0? (hint: consider the OLS closed form solution)**\n","\n","YOUR ANSWER HERE: \n","\n","**2.1.6. What happens to $w$ if $\\lambda$ goes to infinity?**\n","\n","YOUR ANSWER HERE:\n"]},{"cell_type":"markdown","metadata":{"id":"UYdBH6WkUfLt"},"source":["## 2.2 Choosing Lambda\n","\n","In Ridge Regression, $\\lambda$ is inherent to our model, and thus, must be picked by the person creating the model. We refer to this as a **hyperparameter**. What $\\lambda$ works best depends on the data itself. The higher $\\lambda$ is, the more it penalizes complexity, and thus regularizes the output.\n","\n","In this case, we'll show which $\\lambda$ works best by testing against our true function. However, in the real world, where we don't know the true model, it's best to choose hyperparameters through cross-validation.\n","\n","**2.2.1. Run the cell next cell and report the best $\\lambda$.**"]},{"cell_type":"code","metadata":{"id":"mbrP5umf9yR9"},"source":["mses = []\n","lambdas_mse = np.logspace(-5, 5, 10)\n","\n","best_loss = float('inf')\n","best_lambd_mse = None\n","X_test = get_features(7, x_test)\n","X_train = get_features(7, x_train)\n","\n","for lambd in lambdas_mse:\n","    w = ridge(X_train, y_train, lambd)\n","    y_pred_test = X_test@w\n","    loss = mse(y_pred_test, y_test)\n","    mses.append(loss)\n","    if best_loss > loss:\n","        best_loss = loss\n","        best_lambd_mse = lambd\n","\n","plt.plot(lambdas_mse, mses)\n","plt.xscale('log')\n","plt.xlabel('log lambda')\n","plt.ylabel('Testing MSE')\n","plt.show()\n","\n","print(f'Best Loss: {best_loss}, Best Lambda: {best_lambd_mse}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95W2hVnDXLDn"},"source":["# 3 Bias Variance Trade Off \n","\n"]},{"cell_type":"markdown","metadata":{"id":"UgFDrrSbUDJZ"},"source":["In order to further understand why we must tune $\\lambda$, let's look at a very important concept in machine learning: Bias-Variance Trade Off. Please note that the mathematical expressions in this toy example have been simplified to help with understanding and convey the big ideas.  In order to understand this concept, let's start by observing two probability based expressions: $E[|\\hat{w}- w|_2]$ and $Var[|\\hat{w}|_2]$.\n","\n","1.   $E[|\\hat{w}- w|_2]$: This term is the *'bias'* of our model. At a high-level, this term represents the expected distance between our predicted model weights and the true model weights. $|\\cdot|_2$ refers to the *euclidean norm*. $\\hat{w}$ is the predicted weights from ridge regression. $w$ is the true underlying model's weights. Since we are dealing with vectors, $|\\hat{w}- w|_2$ is essentially a 'distance' measure for how far $\\hat{w}$ is from $w$. The $E[\\cdot]$, here, refers to *expectation* of the enclosed quantity. However, the concept of expectation does not need to be understood for this example case. We will instead use *mean* as a proxy for expectation. (Note: The *real* bias term represents how far the predicted $y$ is from the real $y$. However, the math required in analyzing this would be a little more sticky and therefore *our toy* bias term compares the predicted weight with the real weight.)\n","\n","2.   $Var[|\\hat{w}|_2]$: This term is the *'variance'* of our model. This term can be likened to the spread of the predicted model weight. The $Var[\\cdot]$ refers to the variance of the enclosed quantity. Like expectation, the concept of variance is not needed in this example. Instead, we will use the square of the *standard deviation*. (Note: The *real* variance term measures the spread of the predicted $y$. However, our toy variance term just measures the spread of the predicted weights.)\n","\n","Why are these bias and variance expressions important? Intuitively we want both terms to be as close to 0 as possible (both terms are bounded by 0). Let's think about bias first. The closer bias is to zero, the closer our predicted $\\hat{w}$ is to the true model $w$. Next, lets think about variance. A higher  variance implies a greater spread in possible predicted model weights. A greater spread means that the possible predicted model weights will be all over the place yeilding less realiable results. Thus we want the variance to also be close to zero. \n","\n","Now, we understand that we must tune $\\lambda$ in a way that minimizes both $E[|\\hat{w}- w|_2]$ and $Var[|\\hat{w}|_2]$. However, how do we calculate these two quantities? First we fix $\\lambda$ and then repeatedly draw a dataset $(X_i, \\vec{y}_i)$ and compute the associated $\\hat{w}_i$, $r$ times.\n","\n","\n","$$\\hat{w}_i = (X_i^TX_i + \\lambda I)^{-1}X_i^T\\vec{y}_i\\\\ \n","(X_i, \\vec{y}_i), \\quad 1 \\leq i \\leq r\n","$$\n","\n","Then compute the mean and standard deviation associated to the fixed $\\lambda$:\n","$$ E[|\\hat{w}(\\lambda)- w|_2] \\approx \\text{mean}(|\\hat{w}_i- w|_2) = \\frac{1}{r} \\sum_{i = 1}^r |\\hat{w}_i- w|_2\\\\\n","Var[|\\hat{w}(\\lambda)|_2] \\approx [\\text{standard deviation}(|\\hat{w}_i|_2)]^2 = \\frac{1}{r} \\sum_{i = 1}^r \\lvert |\\hat{w}_i|_2 - \\mu \\rvert^2, \\quad\n","\\mu = \\frac{1}{r} \\sum_{i = 1}^r |\\hat{w}_i|_2\n","$$\n","\n","This procedure is repeated over a range of $\\lambda$'s. Note that $w$ is a known value in our experiment because we know what the true model is. However, this is often not the real case in real world machine learning.\n","\n","Now, lets observe how $E[|\\hat{w}(\\lambda)- w|_2]$ and $Var[|\\hat{w}(\\lambda)|_2]$ vary with $\\lambda$. "]},{"cell_type":"markdown","metadata":{"id":"ncbMBQ0TKziB"},"source":["**2.1 Fill in the following code to compute the bias and variance for each $\\lambda$**"]},{"cell_type":"code","metadata":{"id":"bSv7R1KGGuVb"},"source":["def get_train_and_test_data(x_range, degree, function, train_test_split):\n","    X, Y = generate_data(x_range, function, 1, 80)\n","\n","    indicies = np.arange(len(X))\n","    np.random.shuffle(indicies)\n","    split_index = int(train_test_split * len(X))\n","    train_indicies = indicies[:split_index]\n","    test_indicies = indicies[split_index:]\n","\n","\n","    X_train = get_features(degree, X[train_indicies])\n","    Y_train = Y[train_indicies]\n","\n","    X_test = get_features(degree, X[test_indicies])\n","    Y_test = Y[test_indicies]\n","\n","    return X_train, Y_train, X_test, Y_test\n","\n","lambdas_bv = np.logspace(-5, 5, 50)\n","\n","function = lambda x: x**3 + 3*x**2 - 2\n","w_true = np.array([-2, 0, 3, 1, 0, 0, 0, 0])\n","repeat = 600\n","biases = []\n","variances = []\n","X = np.linspace(-3, 1, 101, endpoint=True)\n","np.random.seed(46545645)\n","\n","for lam in tqdm(lambdas_bv):\n","    w_hats_norm = []\n","    distance = []\n","    for _ in range(repeat):\n","        X_train, Y_train, _, _ = get_train_and_test_data(X, 7, function, 0.8)\n","\n","        # TODO: Compute w_hat and then append appropriate values w_hats_norm and distance\n","        ### BEGIN CODE ###\n","        \n","        ### END CODE ###\n","\n","    # TODO: Take the mean and standard devaiation squared of the distance and w_hats_norm array, respectively\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","\n","biases =  np.array(biases)\n","variances = np.array(variances)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MoJQxe5_vr73"},"source":["**2.2 Plot bias and variance as lambda varies.**\n","\n","Make sure to use a log scale for the lambda axis."]},{"cell_type":"code","metadata":{"id":"_LQx5S0eOM5G"},"source":["### BEGIN CODE ###\n","\n","### END CODE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naapQ6D7d0Ak"},"source":["**2.3. How does *bias* change as $\\lambda$ increases? Why does this make intuitive sense (Hint: think about how a larger $\\lambda$ will force the magnitude of $\\hat{w}$ to be smaller)?** \n","\n","YOUR ANSWER HERE:\n","\n","**2.4. How does *variance* change as lambda increases? Why does this make intuitive sense(Hint: similar to the previous question, think about how a larger $\\lambda$ will force the magnitude of $\\hat{w}$ to be smaller)**\n","\n","YOUR ANSWER HERE:\n","\n","**2.5. What is the significance of the word *trade off* in bias variance trade off? (Hint: think about how variance and bias vary as lambda increases)**\n","\n","YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"n8pu2vU9LIwD"},"source":["As stated above, we want both bias and variance to be close to zero. Therefore our objective is to minimize the combination of bias and variance. The exact objective function we want to minimize is $E[|\\hat{w}- w|_2]^2 + Var[|\\hat{w}|_2]$. The reason why the bias term is squared is beyond the scope of this notebook. The fancy way to write this notion in math is shown below:\n","\n","$$\\underset{\\lambda}{\\text{min }} E[|\\hat{w}- w|_2]^2 + Var[|\\hat{w}|_2]\\\\\n","\\underset{\\lambda}{\\text{min }} \\text{bias}^2 + \\text{variance}\n","$$\n","\n","Notice here that the *optimization variable* (the variable which we vary to try to reduce the objective function) is $\\lambda$. This is because the only parameter that we can change in ridge regression is the $\\lambda$ (aside from how we featurize the data).\n"]},{"cell_type":"markdown","metadata":{"id":"tyWwbkf5LOeF"},"source":["**2.6. Plot how $\\text{bias}^2 + \\text{variance}$ varies with $\\lambda$. Also plot MSE vs $\\lambda$ graph from the previous section.**\n","\n","The code will also compute the optimal $\\lambda$ based on where the lowest $\\text{bias}^2 + \\text{variance}$ is achieved. "]},{"cell_type":"code","metadata":{"id":"PBkIXFYlLb0F"},"source":["### BEGIN CODE ###\n","\n","### END CODE ###\n","\n","ymin = np.min(bias_squared_plus_variance)\n","xpos = list(bias_squared_plus_variance).index(ymin)\n","best_lambd_bv = lambdas_bv[xpos]\n","\n","print(f'Best Lambda (according to bias^2 + variance): {best_lambd_bv} ')\n","print(f'Best Lambda (according to MSE): {best_lambd_mse} ')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZU0PF3YLkis"},"source":["**2.7. How does the best lambda according to the $\\text{bias}^2 + \\text{variance}$ compare to the best lambda according to the MSE? (Hint: think about where the 'dip' occurs in both plots)**\n","\n","YOUR ANSWER HERE:\n","\n","**2.8. Why is a high bias considered underfitting? (hint: In general machine learning, high bias implies that the model is likely to miss important patterns. Think about how this relates to the large $\\lambda$ causing a high bias in our case.)**\n","\n","YOUR ANSWER HERE:\n","\n","**2.9. Why is high variance considered overfitting? (hint: High variance generally  that the model is too sensitive to small changes in the training data. Think about how this relates to the case when $\\lambda$ goes close to zero and the ridge regression essentially becomes OLS.)**\n","\n","YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"wACrkMlqnzWa"},"source":["# 4 Eigenvalue Perspective of Ridge Regression"]},{"cell_type":"markdown","metadata":{"id":"ejGp-qvKyvfk"},"source":["**Please note that this section will be more math heavy and serves more as a supplemental section for those who want to further deepen their understanding of ridge regression and its relation to OLS.**"]},{"cell_type":"markdown","metadata":{"id":"lKR7Rv4w3USt"},"source":["### 4.1 Diving a Little Deeper"]},{"cell_type":"markdown","metadata":{"id":"EYDQl6cRSf_e"},"source":["Let's reiterate how OLS is solved again:\n","\n","Optimization Problem: \n","\n","$$\\underset{w}{\\min} \\|y - Xw \\|_2^2$$\n","\n","Closed Form Solution:\n","\n","$$\\hat{w} = (X^TX)^{-1}X^T\\vec{y}$$\n","\n","Let's dig a little deeper into the matrix $X^TX$ from the closed-form solution, but first let's build some intuition."]},{"cell_type":"markdown","metadata":{"id":"MofVtutFWB7L"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"btwRtcluSnXM"},"source":["**4.1.1.** Suppose each data point in $X$ was 1 dimensional.\n","\n","The matrix multiplication $X^T X$ would be $1 \\times n$ by $n \\times 1$, which outputs a $1 \\times 1$ matrix, making $X^T X$ a scalar in this case.\n","\n","**What values of $X^T X$ may \"mess up\" the calculation of $\\hat{w}$ in the closed-form solution? *(Hint: What happens if $X^TX$ is 0 or close to 0)***"]},{"cell_type":"markdown","metadata":{"id":"cZc7v1krT9Jg"},"source":["**SOLUTION:** If $X^TX$ is 0 or very close to 0, then $(X^TX)^{-1}$, or $\\frac{1}{X^TX}$, would end up exploding, since $\\frac{1}{x}$ would approach infinity as $x$ approaches 0."]},{"cell_type":"markdown","metadata":{"id":"xkI5kLyOWDkE"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"TOtt9kEGn4S6"},"source":["Let's expand this issue a more general case, when $X^TX$ is a matrix. But before that let's do a quick review on eigenvalues and eigenvectors.\n","\n","In EECS 16A, you learned about eigenvalues and eigenvectors of a matrix, where the $i$th eigenvalue-eigenvector pair $(\\lambda_i, \\vec{v}_i)$ of some matrix $A$ exhibits the following behavior:\n","\n","$$A\\vec{v}_i = \\lambda_i \\vec{v}_i$$\n","\n","You also learned about diagonalization, where if a $n \\times n$ matrix has $n$ linearly independent eigenvectors, then you can express some matrix $A$ as:\n","\n","$$A = V \\Lambda V^{-1}$$\n","\n","Where $V = \\begin{bmatrix} \\vec{v}_1 & \\cdots & \\vec{v}_n \\end{bmatrix}$ is the matrix where each column is an eigenvector of the matrix, and $\\Lambda$ is a diagonal matrix containing the eigenvalues, i.e.:\n","\n","$$\\Lambda = \\begin{bmatrix} \n","\\lambda_1 & 0 & \\cdots & 0 \\\\\n","0 & \\lambda_2 & \\cdots & 0 \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","0 & 0 & \\cdots & \\lambda_n\n","\\end{bmatrix}$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"acIYBrSSXIF-"},"source":["Now that we have done a quick review of eigenvalues and eigenvectors, we are now ready to analyze $X^T X$.\n","\n","Fortunately, $X^TX$ (and any matrix that of similar form) falls into a category of special matrices known as positive-semidefinite matrices, and has two properties that we will utilize:\n","\n","* $X^TX$ is symmetric (verify with yourself why this is true), and thus is diagonalizable.\n","* Being positive semidefinite, $X^TX$ has eigenvalues that positive or zero, that is $\\lambda_i \\geq 0, \\forall i$.\n","\n","As mentioned above $X^T X$ is symmetric and thus diagonalizable, which is great news! We can diagonalize $X^T X$ in the following manner.\n","\n","$$X^T X = V \\Lambda V^{-1}$$\n","\n","With this decomposition, we can now analyze the eigenvalues of $X^T X$ as they are located in $\\Lambda$, and this will help us reveal more about the behavior of OLS."]},{"cell_type":"markdown","metadata":{"id":"Brdn-6eHbAUt"},"source":["Now we are ready to examine $(X^T X)^{-1}$.\n","\n","Based on the scalar case, we know that taking the inverse, aka taking the reciprocal can lead to some issues, so let's see if something similar shows up. Using the diagonalization of $X^T X$, we can construct an inverse in terms of $V$ and $\\Lambda$.\n","\n","We know that $V$ is comprised of linearly independent eigenvectors, as this is a requirement for diagonalization, which implies that $V$ is invertible. Thus, we can left multiply $X^T X$ by $V^{-1}$ to get the following result:\n","\n","$$V^{-1} (X^T X) = V^{-1} (V \\Lambda V^{-1}) = \\Lambda V^{-1}$$\n","\n","Since $\\Lambda$ is a diagonal matrix, its inverse is a diagonal matrix where each non-zero entry is the reciprocal of the corresponding entry in the original matrix. In the case of $\\Lambda$, we have:\n","\n","$$\n","\\Lambda^{-1} = \\begin{bmatrix} \n","\\frac{1}{\\lambda_1} & 0 & \\cdots & 0 \\\\\n","0 & \\frac{1}{\\lambda_2} & \\cdots & 0 \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","0 & 0 & 0 & \\frac{1}{\\lambda_n}\n","\\end{bmatrix}\n","$$\n","\n","We can then left multiply what we currently have by $\\Lambda^{-1}$:\n","\n","$$\\Lambda^{-1} (V^{-1} X^T X) = \\Lambda^{-1} (\\Lambda V^{-1}) = V^{-1}$$\n","\n","Finally, we can left multiply by $V$ again to obtain the identity matrix:\n","\n","$$V(\\Lambda^{-1} V^{-1} X^T X) = V(V^{-1}) = I$$\n","\n","Thus we can conclude that\n","\n","$$(X^T X)^{-1} = V^{-1} \\Lambda^{-1} V$$"]},{"cell_type":"markdown","metadata":{"id":"BmwKkXTcWKPN"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"3D9TNGKzrGqM"},"source":["**4.1.2. Now that we know some more details about the inverse of $X^TX$, what problems may arise when calculating this matrix? *(Hint: Taking the reciprocal of a scalar $X^TX$ was a \"weak point\" in the process, is there something similar in the matrix case? What does being positive semi-definite mean for the eigenvalues of $X^TX$?)***"]},{"cell_type":"markdown","metadata":{"id":"chpWSUfnfbuG"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"OxNixMqyWLPl"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"rBA_6c_IVbub"},"source":["Now that you have a better understanding of what is going on mathematically behind the scenes, let's examine the behavior of the data matrix in action!"]},{"cell_type":"markdown","metadata":{"id":"_vD3tKMdS7lr"},"source":["## 4.2 Visualizing Eigenvalues"]},{"cell_type":"markdown","metadata":{"id":"waCX5mqnrg--"},"source":["Let's visualize the eigenvalues of $X^T X$\n","\n","For visualization purposes, we will resample $x$ values in the interval $[-1, 1]$. We then will featurize $x$ into $X$ with polynomial features at varying degrees.\n","\n","**4.2.1. Implement the code to find the eigenvalues of a matrix. *(Hint: What numpy function can do this for us?)***"]},{"cell_type":"code","metadata":{"id":"uQKlkLE7v0oH"},"source":["def eigenvalues(X):\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","    \n","    return eigenvals"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zc9liZh-XTLF"},"source":["**4.2.2 Run the following cell to see the eigenvalues of $X^T X$ at varying polynomial degrees. Pay attention to the scaling of the y-axis.**"]},{"cell_type":"code","metadata":{"id":"qRgakZRpXL3s"},"source":["np.random.seed(0)\n","x_new = np.random.uniform(-1, 1, 50)\n","\n","def plot_OLS_eigenvalues(x, degree):\n","    X = get_features(degree, x)\n","    eigs = eigenvalues(X.T @ X)\n","    plt.plot(np.arange(1, len(eigs) + 1), eigs, 'o', label=\"Eigenvalues\")\n","    plt.yscale('log')\n","    plt.xlim((0, 76))\n","    plt.ylim((1e-20, 1e4))\n","    plt.legend()\n","\n","deg_slider = widgets.IntSlider(value=1,\n","                               min=1,\n","                               max=75,\n","                               step=1,\n","                               description=\"Degree\")\n","\n","interactive(plot_OLS_eigenvalues, x=fixed(x_new), degree=deg_slider)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dTWbYN2qaOPp"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"o3gCOlbMv1p9"},"source":["**4.2.3. Comment on what you observe. Does what you see match the cause for issues that may arise from taking the inverse of $X^T X$ in OLS?**"]},{"cell_type":"markdown","metadata":{"id":"qjcMCZZ1X0to"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"KH3CZWC2ZDqN"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"P1QkGu2iwXdv"},"source":["Now let's examine the solution to ridge regression:\n","\n","Optimization Problem: \n","\n","$$\\underset{w, \\lambda}{\\min} \\|y - Xw \\|_2^2 + \\lambda \\|w\\|_2^2$$\n","\n","Closed Form Solution: \n","\n","$$\\hat{w} = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$$"]},{"cell_type":"markdown","metadata":{"id":"hrpgz7yYwmYE"},"source":["Let's visualize the eigenvalues of $X^T X + \\lambda I$.\n","\n","**4.2.4 Run the cell below, where the features of $X$ is fixed to be at dimension 60, and we vary $\\lambda$.**"]},{"cell_type":"code","metadata":{"id":"yvz-MW2NwloX"},"source":["X_new = get_features(60, x_new)\n","eigs_ols = eigenvalues(X_new.T @ X_new)\n","\n","def plot_ridge_eigenvalues(X, lambd):\n","    eigs_ridge = eigenvalues(X.T @ X + lambd * np.eye(X.shape[1]))\n","\n","    plt.plot(np.arange(1, len(eigs_ols) + 1), eigs_ols, 'o', label=\"OLS\")\n","    plt.plot(np.arange(1, len(eigs_ridge) + 1), eigs_ridge, 'o', label=\"Ridge\")\n","    plt.yscale('log')\n","    plt.ylim((1e-20, 1e4))\n","    plt.legend()\n","\n","lambd_slider = widgets.FloatLogSlider(value=1e-10,\n","                                      min=-20, \n","                                      max=1, \n","                                      step=1, \n","                                      description=\"lambda\")\n","\n","interactive(plot_ridge_eigenvalues, X=fixed(X_new), lambd=lambd_slider)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6Y63lZki0E3"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"xyIis-3hwygS"},"source":["**4.2.5 Comment on what you observe as you change $\\lambda$. How do you think the calculation will differ between Ridge Regression and OLS?**"]},{"cell_type":"markdown","metadata":{"id":"FqlB7eKKi1EG"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"d2QBY6gfjFFd"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"GXg3Y3NbxUJA"},"source":["We can now see ridge regression in another perspective: it serves as a way to make OLS a more robust process by controlling the eigenvalues of the $X^T X$ matrix with a lower bound, stabilizing the calculation of the weight vector."]},{"cell_type":"markdown","metadata":{"id":"cE4Cc7qD3c5i"},"source":["### 4.3 Ridge Regression as a High-Pass Filter"]},{"cell_type":"markdown","metadata":{"id":"tB-JpfeQzFrx"},"source":["In EECS 16B, you have learned about RLC circuits and the phasor space, which was used to help analyze circuits that receive sinusoidal voltages as an input.\n","\n","(Based on Fall 2020, freq response occurs in week 5, but students already know about the phasor space, in this case it would be better to walk through a high pass filter and introduce the $H$ function).\n","\n","We will walk through a specific type of RLC circuit called a high-pass filter and see how the behavior of this circuit relates to the eigenvalues in Ridge Regression."]},{"cell_type":"markdown","metadata":{"id":"ledMYpj41IXY"},"source":["(In the case where frequency responses and bode plots have already been introduced)\n","\n","In particular, you learned about how you can create filters for certain frequencies, where a high-pass filter lets high frequences through, a low pass filter lets low frequencies through, and a band-pass only allows a range of frequencies to pass through.\n","\n","Let's reanalyze the high-pass and low-pass filters and generate plots for their frequency responses."]},{"cell_type":"markdown","metadata":{"id":"YHXmoHyq1sk-"},"source":["Take a look at the following circuit below:"]},{"cell_type":"markdown","metadata":{"id":"vUjfZ4XNT_X-"},"source":["![CR circuit.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"e8Pa4C8L10RU"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"RF4QaVBA13Ep"},"source":["**4.3.1.** Suppose we had a sinusoidal input voltage at some frequency $\\omega$, (i.e. $v_{in}(t) = \\operatorname{Re} \\left[V_{in} e^{j\\omega t}\\right]$), and a sinusoidal output voltage $v_{out}(t) = \\operatorname{Re} \\left[V_{out} e^{j\\omega t}\\right]$. The capacitor has a capacitance of $C$ and the resistor has a resistance of $R$.\n","\n","**Convert the above circuit to the phasor domain. If $v_{in}$ and $v_{out}$ are the voltages in the domain of real numbers, what is their phasor representation *(Don't overthink this!)* What are the impedances of the capacitor and the resistor?**"]},{"cell_type":"markdown","metadata":{"id":"H22k-6FS4jnR"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"cU8LLy7g5WCI"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"UScyQpwcEp9N"},"source":["**4.3.2.** Now that we have converted the components of the circuit into the phasor domain, we will now derive an important characteristic function called the *transfer function*:\n","\n","$$H(\\omega) = \\frac{V_{out}}{V_{in}}$$\n","\n","**Quickly explain what the transfer function represents. Derive the transfer function for this particular circuit, which will be of the form $\\frac{1}{1 + h(\\omega)}$, where $h(\\omega)$ will be some function of $\\omega$. *(Hint: Start with the impedances $Z_R$ and $Z_C$ and write out $H$ in terms of them. Then plug in the values for $Z_R$ and $Z_C$ you found in the previous problem).***"]},{"cell_type":"markdown","metadata":{"id":"DkZEAgV8Gd1l"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"ugDZ1Ic4GeXc"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"_cwSOSrxJXSV"},"source":["**4.3.3.** Now let's quickly analyze how the magnitude of the transfer function, and thus the magnitude of the input and output voltages, behave.\n","\n","**What do you think is the largest magnitude $H(\\omega)$ can take? At what value of $\\omega$ do you think this will happen? Explain what that means in terms of the input and output voltages. \n","What is the smallest magnitude $H(\\omega)$ can take, and what value will $\\omega$ be at? Explain what that means in terms of the input and output voltages.**"]},{"cell_type":"markdown","metadata":{"id":"QWoJ0R-XKkyf"},"source":["**SOLUTION:** We see that the second denominator term will be imaginary, so it will never cancel out any real component in $H(w)$. Thus, in order to maximize $H(\\omega)$, we need the imaginary component to be 0. This will occur when $\\omega$ approaches infinity. \n","\n","$$H(\\infty) = \\frac{1}{1 + j\\frac{1}{\\infty}} = \\frac{1}{1 + 0j} = 1 \\implies |H(\\infty)| = 1$$\n","\n","In this scenario, a ratio of 1 means that the magnitude of the output signal is the same as the input signal, or in other words the input signal is completely preserved in the output.\n","\n","In order minimize $H(\\omega)$ we want the imaginary component to be as large as possible. This will occur when $\\omega = 0$. \n","\n","$$H(0) = \\frac{1}{1 + j\\frac{1}{0}} = \\frac{1}{1 + \\infty j}$$\n","\n","$$|H(0)| = \\left|\\frac{1}{1 + \\infty j}\\right| = \\frac{1}{|1 + \\infty j|} = \\frac{1}{\\sqrt{1^2 + \\infty^2}} \\approx \\frac{1}{\\infty} = 0$$\n","\n","In this scenario, a ratio of 0 means that the magnitude of the output signal is 0, or in other words, none of the input signal was preserved in the output."]},{"cell_type":"markdown","metadata":{"id":"KgSCRA0UL2fk"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"GBOOBoWAONdl"},"source":["**4.3.4 Explain why this particular circuit would be called a high-pass filter.**"]},{"cell_type":"markdown","metadata":{"id":"3UlBx9L2Od5_"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"uTmOg5hUO7Pu"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"ueCElZFtO9M2"},"source":["Now that we have an intuition of how the transfer function behaves, let's visualize it. A plot of the maginitude of the transfer function relative to the frequency of the input voltage is known as a *Bode Plot*.\n","\n","**4.3.5. Run the next three cells below to take a look at the Bode Plot of our circuit. Play around with the slider that changes the value of $\\frac{1}{RC}$. Again pay attention to the scale of both x and y axes.**"]},{"cell_type":"code","metadata":{"id":"yG-1lSMMzHI-"},"source":["def high_pass(omega, RC):\n","    return 1 / (1 + 1 / (1j * omega * RC))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59Lj2KeQW0D3"},"source":["omegas = np.logspace(-5, 5, 100)\n","\n","def plot_high_pass(omegas, RC):\n","    magnitudes = np.absolute(high_pass(omegas, 1/RC))\n","    plt.plot(omegas, magnitudes)\n","    plt.xscale('log')\n","    plt.yscale('log')\n","    plt.ylim((1e-6, 1e1))\n","    plt.grid()\n","\n","high_pass_slider = widgets.FloatLogSlider(value=1,\n","                                            min=-5,\n","                                            max=5,\n","                                            step=0.25,\n","                                            description=\"1/RC\")\n","\n","interactive(plot_high_pass, omegas=fixed(omegas), RC=high_pass_slider)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDXx04gzmMe7"},"source":["def plot_high_pass_2(omegas, RC):\n","    magnitudes = np.absolute(high_pass(omegas, 1/RC))\n","    plt.plot(omegas, magnitudes)\n","    plt.xscale('log')\n","    plt.ylim((-.25, 1.25))\n","    plt.grid()\n","\n","high_pass_slider = widgets.FloatLogSlider(value=1,\n","                                            min=-5,\n","                                            max=5,\n","                                            step=0.25,\n","                                            description=\"1/RC\")\n","\n","interactive(plot_high_pass_2, omegas=fixed(omegas), RC=high_pass_slider)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89k9Gsgbaf4_"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"dV6-ERmgahPC"},"source":["**4.3.6. In the Bode plot, at what frequencies are the voltages kept and when is it \"silenced\"? What seems to be the threshold for this behavior?**"]},{"cell_type":"markdown","metadata":{"id":"oaSmD7VCbRP6"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"lcSKoX8EbfHU"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"RM_hbRQIbft6"},"source":["**4.3.7. Compare this with the plot of eigenvalues. What part of the $H$ function do eigenvalues represent? What part of the $H$ function does $\\lambda$ represent? *(Hint: What exactly does the $H$ function calculate? Can you use this information to relate it back to eigenvalues in Ridge?)***"]},{"cell_type":"markdown","metadata":{"id":"MgbOxOl6b2W5"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"Y22Y7fLadedr"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"hceh1VhgdfyK"},"source":["Now you can see another perspective of Ridge Regression: as a high pass filter on eigenvalues! We hope you have a better intuition on how the eigenvalue plots work in relation to $\\lambda$, and how it can help improve the regression performance compared to OLS."]},{"cell_type":"markdown","metadata":{"id":"WNETABJaSWCO"},"source":["# 5 Alternative Solution to Ridge and Fake Data/Features perspectives"]},{"cell_type":"markdown","metadata":{"id":"3Yl34XoZScGb"},"source":["## 5.1 Alternative Solution to Ridge Regression\n","\n","An important detail to note in OLS is that the closed-form solution $w = (X^TX)^{-1} X^Ty$ is designed for a regression problem where the data matrix is *tall*, or has more data points than features ($n > d$)."]},{"cell_type":"markdown","metadata":{"id":"r0mDgjBGVMlt"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Qnmq8CFVVNYD"},"source":["**5.1.1. Sanity Check:** Suppose that the features of a tall data matrix $X$ were linearly independent. **Comment on the existence of a solution. How does that tie into OLS?**"]},{"cell_type":"markdown","metadata":{"id":"edbII2G-Vc4t"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"Szn7eOhfV5GP"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"NPj5WnPRV6qT"},"source":["**5.1.2** Now suppose that $X$ is a square matrix and has linearly independent columns. **Comment on the existence of a solution. How would you find the solution?**"]},{"cell_type":"markdown","metadata":{"id":"2GjLmq72WERf"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"u4cBVToAWO_x"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"_obvtheHWQEd"},"source":["**5.1.3.** Now suppose that $X$ is a wide matrix, where it has more features than data points ($n < d$). **Comment on the existence of a solution. *(Hint: Are the columns of $X$ linearly independent?)***"]},{"cell_type":"markdown","metadata":{"id":"zc8_wu4uXBSh"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"BN01Cm_hX8Wx"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"HY9C3VoKX9Si"},"source":["Now you know the three scenarios for the shape the data matrix can take on, let's now focus on the wide matrix case more. \n","\n","**5.1.4. Since there can be infinite solutions $w$ to the system $Xw = y$ if $X$ is wide, what would be the \"best\" $w$ in this case. *(Hint: Think about ridge regression, what was it trying to minimize?)***"]},{"cell_type":"markdown","metadata":{"id":"v_snVu62m2l5"},"source":["YOUR ANSWER HERE:"]},{"cell_type":"markdown","metadata":{"id":"iyUiM7cVnQXY"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"_Y1VYTQInRVY"},"source":["Now that we understand what the goal with solving regression problem with wide matrices is, let's formally define the problem and the closed-form solution:\n","\n","Optimization Problem:\n","\n","$$\\underset{w}{\\min} \\|w\\|^2_2 \\text{ s.t. } Xw = y$$\n","\n","Closed-Form Solution:\n","\n","$$w = X^T(XX^T)^{-1}y$$\n","\n","This looks very similar to the OLS solution! It turns out this solution is known as the *minimum-norm solution* and later in EECS 16B you will learn how this solution is derived.\n","\n","Similarily, if we were to add a ridge pentalty to this minimum-norm solution, then we would arrive at an alternative closed-form solution for Ridge Regression:\n","\n","$$w = X^T(XX^T + \\lambda I)^{-1}y$$\n","\n","You don't need to fully understand the significance of this alternative solution for ridge regression right now, but it is useful to notice that the matrix multiplication of $XX^T$ consists of only dot products between $\\vec{x}_i$ feature vectors. This property connects very well with Kernels, a topic you will learn about in a future lesson.\n","\n","**5.1.5. Fill in the code below and run the cell to verify that the alternative closed-form solution for ridge regression gives us the same result:**"]},{"cell_type":"code","metadata":{"id":"dclYX9mcqiit"},"source":["# Generating the polynomial toy model data again\n","x_range = np.linspace(-3, 1, 101, endpoint=True)\n","func = lambda x: x**3 + 3*x**2 - 2\n","np.random.seed(123)\n","x, y = generate_data(x_range, func, 0.4, 80)\n","\n","N = 40\n","x_train = x[:N] \n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","X_train = get_features(D, x_train)\n","\n","def ridge_alternative(X, y, lambd=0.1):\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","    return w\n","\n","lambd = 0.1\n","w_ridge = ridge(X_train, y_train, lambd)\n","w_ridge_alternative = ridge_alternative(X_train, y_train, lambd)\n","print(f'w_ridge: {w_ridge}')\n","print(f'w_alternative: {w_ridge_alternative}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Au22zrHMw_Kp"},"source":["## 5.2 Fake Data and Fake Features Perspective\n","\n","We are going to introduce two final perspectives on Ridge Regression, which are the fake data and fake features perspectives. More specifically, we will see that the fake data perspective will net us the standard closed-form solution for ridge regression while the fake features perspective will net us the alternative solution to ridge regression.\n","\n","**5.2.1. Fake Data Perspective**\n","\n","Given that we have a properly constructed $X$ matrix and $\\vec{y}$ vector, let us add fake data points to $X$ and $\\vec{y}$ such that:\n","\n","$$\\hat{X} = \\begin{bmatrix}\n","X\\\\\n","\\sqrt{\\lambda}I\n","\\end{bmatrix}$$\n","\n","$$\\hat{y}=\\begin{bmatrix}\n","\\vec{y}\\\\\n","0\n","\\end{bmatrix}$$\n","\n","Show that the closed-form ols solution using the augmented $\\hat{X}$ matrix and $\\hat{y}$ vector will net us the closed-form solution for ridge regression:\n","\n","YOUR ANSWER HERE:\n","\n","\n","**5.2.2. Fill in the code below and run the cell to see that the fake data perspective gives us the same result as ridge**"]},{"cell_type":"code","metadata":{"id":"tUiLtLv2IpfU"},"source":["def ridge_fake_data(X, y, lambd = 0.1):\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","    w = ols(X_hat, y_hat)\n","    return w;\n","\n","X_train = get_features(D, x_train)\n","lambd = 0.1\n","w_ridge = ridge(X_train, y_train, lambd)\n","w_fake_data = ridge_fake_data(X_train, y_train, lambd)\n","print(f'w_ridge: {w_ridge}')\n","print(f'w_fake_data: {w_fake_data}')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k9ZPZBiVGmlz"},"source":["**5.2.3. Fake Features Perspective**\n","Let's augment the data matrix again, except this time we are adding fake features such that:\n","\n","$$\\hat{X} = \\begin{bmatrix}\n","X \\sqrt{\\lambda}I\n","\\end{bmatrix}$$\n","\n","Notice that the $\\hat{X}$ matrix is wide now, so we need to use the minimum-norm solution instead of OLS. In addition the weight vector we find using the minimum-norm solution will actually have two components: one for the original features and one for the fake features. We will show this decomposition by defining the weight vector from the minimum-norm solution as $\\begin{bmatrix}\n","\\hat{w}\\\\\n","\\hat{\\epsilon}\n","\\end{bmatrix}$. Show that the minimum-norm solution with the augmented $\\hat{X}$ matrix will net us the same $\\hat{w}$ as the alternative closed-form solution for ridge regression:\n","\n","YOUR ANSWER HERE:\n","\n","**5.2.4. Fill in the code below and run the cell to see that the fake features perspective gives us the same result as ridge**"]},{"cell_type":"code","metadata":{"id":"51kAfwwYdMjf"},"source":["def ridge_fake_features(X, y, lambd = 0.1):\n","    X_hat = np.hstack((X, np.sqrt(lambd)*np.eye(X.shape[0])))\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","    return w;\n","\n","X_train = get_features(D, x_train)\n","lambd = 1\n","w_ridge_alternative = ridge_alternative(X_train, y_train, lambd)\n","w_fake_data = ridge_fake_features(X_train, y_train, lambd)\n","print(f'w_ridge_alternative: {w_ridge_alternative}')\n","print(f'w_fake_data: {w_fake_data}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itTP87SpB15t"},"source":["# 6 Sklearn\n","\n","Finally, we will run through some exercises to get familiar with Sklearn, a machine learning library in python. In most of the practical settings, we won't need to write our own featurization method or ridge regression method as those are often provided for us in common libraries like Sklearn.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F2PlgWe3KzF_"},"source":["## 6.1 Sklearn on the Polynomial Toy Model dataset\n","First, let's reimplement ridge regression on our polynomial toy model using the PolynomialFeatures and Ridge classes of Sklearn. We will first use sklearn's PolynomialFeatures class to lift the data into a degree 7 polynomial, then perform ridge regression on the dataset and report the training error and test error. Here's some useful documentation to get you started:\n","\n","1. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n","2. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n","\n","\n","**6.1.1. Implement the code below to generate a plot of the training and testing error over a range of lambdas and report the lambda with the best test error.**"]},{"cell_type":"code","metadata":{"id":"MsCrasK1CAi1"},"source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import Ridge\n","\n","# Regenerating the polynomial toy model data\n","x_range = np.linspace(-3, 1, 101, endpoint=True)\n","func = lambda x: x**3 + 3*x**2 - 2\n","np.random.seed(123)\n","x, y = generate_data(x_range, func, 0.4, 80)\n","# Training and validation set split\n","N = 40\n","x_train = x[:N]\n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","D = 7\n","lambdas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n","mses_train = []\n","mses_test = []\n","best_mse = float('inf')\n","best_lambd = None\n","\n","# TODO: generate the polynomial features for X_train and X_test using PolynomialFeatures\n","### BEGIN CODE ###\n","\n","### END CODE ###\n","\n","for lambd in lambdas:\n","    # TODO: Train a Ridge model on the polynomial features and evaluate the mse for both training and test sets\n","    # HINT: the method mse() might be useful here\n","    ### BEGIN CODE ###\n","    \n","    ### END CODE ###\n","\n","plt.plot(lambdas, mses_train, label='Train')\n","plt.plot(lambdas, mses_test, label='Test')\n","plt.xscale('log')\n","plt.xlabel('log lambda')\n","plt.ylabel('MSE')\n","plt.legend()\n","plt.show()\n","\n","print(f'Best Loss: {best_loss}, Best Lambda: {best_lambd}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JE-iZ1DWCu80"},"source":["Compare this error chart with error chart we generated earlier without Sklearn. If you did the coding portion correctly, the results should be similar - which is expected."]},{"cell_type":"markdown","metadata":{"id":"DymY4u88BXVe"},"source":["## 6.2 Sklearn on a different dataset\n","Now that you've had an introduction to the Ridge and PolynomialFeature classes of the Sklearn library, let's practice implementing ridge regression using Sklearn on a different dataset.\n","\n","**6.2.1 Run the cell below to visualize our new dataset:**"]},{"cell_type":"code","metadata":{"id":"--2eu16B5-j2"},"source":["x_range = np.linspace(-1, 1, 101, endpoint=True)\n","func = lambda x: np.exp(x)\n","np.random.seed(12)\n","x, y = generate_data(x_range, func, .4, 80)\n","\n","# Training and validation set split, we'll just use 50/50 in this case\n","N = 40\n","x_train = x[:N]\n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","# Plot the true function and training data\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.plot()\n","plt.legend()\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMsNHuqQDzwI"},"source":["Again, let's try to fit this data with a degree 7 polynomial with ridge regression.\n","\n","**6.2.2. Using the Sklearn library, write code below to visualize the training and testing errors over a range of lambdas and report the best lambda.*(hint: feel free to reference 5.1.1 to get you started)***"]},{"cell_type":"code","metadata":{"id":"_Cj3IMwm_Nfo"},"source":["### BEGIN CODE ###\n","\n","### END CODE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iA15xYpgD7P9"},"source":["**6.2.3. Fill in the best lambda below and run the cell to visualize our learned function. Do you think this was a good choice of lambda? Try other values of lambda as well and report your observations.**\n","\n","YOUR ANSWER HERE:"]},{"cell_type":"code","metadata":{"id":"49VBVDWl_iTg"},"source":["LAMBD = 1 # Edit this\n","sklearn_ridge = Ridge(alpha=LAMBD)\n","sklearn_ridge.fit(X_train, y_train)\n","X_range = poly.fit_transform(np.array(x_range).reshape(len(x_range),1))\n","y_range_pred = sklearn_ridge.predict(X_range)\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_range, y_range_pred, label=f'Predicted Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1saLgyZGgfH"},"source":["## 6.3 Additional Datasets for Practice\n","\n","**6.3.1. Here are some more datasets you can use to practice implementing Ridge Regression with the Sklearn library. Feel free to explore other degrees of polynomial features as well.**\n"]},{"cell_type":"code","metadata":{"id":"3iZ11KJ7At0A"},"source":["dataset = 'sine' # Dataset 1: Sine Function\n","# dataset = 'sqrt' # Dataset 2: Square Root Function\n","# dataset = 'inverse' # Dataset 3: Inverse Function\n","\n","if dataset == 'sine':\n","    x_range = np.linspace(-2, 2, 101, endpoint=True)\n","    func = lambda x: np.sin(x)\n","    np.random.seed(12)\n","    x, y = generate_data(x_range, func, .4, 80)\n","\n","if dataset == 'sqrt':\n","    x_range = np.linspace(0, 10, 101, endpoint=True)\n","    func = lambda x: np.sqrt(x)\n","    np.random.seed(12)\n","    x, y = generate_data(x_range, func, .4, 80)\n","\n","if dataset == 'inverse':\n","    x_range = np.linspace(.5, 1.5, 101, endpoint=True)\n","    func = lambda x: 1/x\n","    np.random.seed(12)\n","    x, y = generate_data(x_range, func, .2, 80)\n","    \n","# Training and validation set split, using 50/50 for now\n","N = 40\n","x_train = x[:N]\n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","\n","# Plot the true function and training data\n","plt.plot(x_range, func(x_range), label='True Function')\n","plt.plot(x_train, y_train, 'o', label='Training Data')\n","plt.plot()\n","plt.legend()\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[]}]}