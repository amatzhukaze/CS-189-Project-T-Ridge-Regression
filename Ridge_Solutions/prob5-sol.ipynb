{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prob5-sol.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WNETABJaSWCO"},"source":["# 5 Alternative Solution to Ridge and Fake Data/Features perspectives"]},{"cell_type":"markdown","metadata":{"id":"H20UL5P3tHYH"},"source":["**STOP: If you have not completed Problem 2, please do that first!**"]},{"cell_type":"code","metadata":{"id":"yqKmGH71Hcxi"},"source":["import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"db9J2XjHIM1i"},"source":["We will need some functions from previous problems."]},{"cell_type":"code","metadata":{"id":"AwVd-rMIHnle"},"source":["def generate_data(x_range, func, sigma=1, n=80):\r\n","    y_range = np.array([func(x) + np.random.normal(0, sigma) for x in x_range])\r\n","    random_indicies = np.arange(len(x_range))\r\n","    np.random.shuffle(random_indicies)\r\n","    x = x_range[random_indicies[:n]]\r\n","    y = y_range[random_indicies[:n]]\r\n","    return x, y\r\n","\r\n","def get_features(d, x_scalars):\r\n","    X = []\r\n","    for x in x_scalars:\r\n","        X.append([x**i for i in range(d+1)])\r\n","    return np.array(X)\r\n","\r\n","def ols(X, y):\r\n","    ### BEGIN CODE ###\r\n","    w_hat = np.linalg.inv(X.T@X)@X.T@y\r\n","    ### END CODE ###\r\n","    return w_hat\r\n","\r\n","def ridge(X, y, lambd=1):\r\n","    return np.linalg.inv(X.T@X + lambd*np.eye(X.shape[1]))@X.T@y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Yl34XoZScGb"},"source":["## 5.1 Alternative Solution to Ridge Regression\n","\n","An important detail to note in OLS is that the closed-form solution $w = (X^TX)^{-1} X^Ty$ is designed for a regression problem where the data matrix is *tall*, or has more data points than features ($n > d$)."]},{"cell_type":"markdown","metadata":{"id":"r0mDgjBGVMlt"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Qnmq8CFVVNYD"},"source":["**5.1.1. Sanity Check:** Suppose that the features of a tall data matrix $X$ were linearly independent. **Comment on the existence of a solution. How does that tie into OLS?**"]},{"cell_type":"markdown","metadata":{"id":"edbII2G-Vc4t"},"source":["**SOLUTION:** Since there are more data points than features, in linear algebra terms there are more equations than variables. This would imply that a solution may not exist. In the equation $Xw = y$, OLS finds the solution in the column space of $X$ that is closest to $y$."]},{"cell_type":"markdown","metadata":{"id":"Szn7eOhfV5GP"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"NPj5WnPRV6qT"},"source":["**5.1.2** Now suppose that $X$ is a square matrix and has linearly independent columns. **Comment on the existence of a solution. How would you find the solution?**"]},{"cell_type":"markdown","metadata":{"id":"2GjLmq72WERf"},"source":["**SOLUTION:** Since $X$ is square and linearly independent, we can simply use the inverse to find $w$:\n","\n","$$w = X^{-1} y$$\n","\n","The existence of an inverse also implies that the solution is unique."]},{"cell_type":"markdown","metadata":{"id":"u4cBVToAWO_x"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"_obvtheHWQEd"},"source":["**5.1.3.** Now suppose that $X$ is a wide matrix, where it has more features than data points ($n < d$). **Comment on the existence of a solution. *(Hint: Are the columns of $X$ linearly independent?)***"]},{"cell_type":"markdown","metadata":{"id":"zc8_wu4uXBSh"},"source":["**SOLUTION:** Since $X$ is a wide matrix, the columns cannot be linearly independent (verify why this is true, what can the maximum rank of $X$ be?). Since the columns are linearly dependent, if a solution exists then there must be infinite solutions, since once you have found a solution, you are able to rearrage some of the weights on each column such that you get an equivalent solution. If the equations are inconsistent, then there are no solutions."]},{"cell_type":"markdown","metadata":{"id":"BN01Cm_hX8Wx"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"HY9C3VoKX9Si"},"source":["Now you know the three scenarios for the shape the data matrix can take on, let's now focus on the wide matrix case more. \n","\n","**5.1.4. Since there can be infinite solutions $w$ to the system $Xw = y$ if $X$ is wide, what would be the \"best\" $w$ in this case. *(Hint: Think about ridge regression, what was it trying to minimize?)***"]},{"cell_type":"markdown","metadata":{"id":"v_snVu62m2l5"},"source":["**SOLUTION:** In ridge regression, we applied a penalty on the norm of the vector $w$. In this case the similar logic applies and the ideal solution $w$ would be the one with minimum norm."]},{"cell_type":"markdown","metadata":{"id":"iyUiM7cVnQXY"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"_Y1VYTQInRVY"},"source":["Now that we understand what the goal with solving regression problem with wide matrices is, let's formally define the problem and the closed-form solution:\n","\n","Optimization Problem:\n","\n","$$\\underset{w}{\\min} \\|w\\|^2_2 \\text{ s.t. } Xw = y$$\n","\n","Closed-Form Solution:\n","\n","$$w = X^T(XX^T)^{-1}y$$\n","\n","This looks very similar to the OLS solution! It turns out this solution is known as the *minimum-norm solution* and later in EECS 16B you will learn how this solution is derived.\n","\n","Similarily, if we were to add a ridge pentalty to this minimum-norm solution, then we would arrive at an alternative closed-form solution for Ridge Regression:\n","\n","$$w = X^T(XX^T + \\lambda I)^{-1}y$$\n","\n","You don't need to fully understand the significance of this alternative solution for ridge regression right now, but it is useful to notice that the matrix multiplication of $XX^T$ consists of only dot products between $\\vec{x}_i$ training data. This property connects very well with Kernels, a topic you will learn about in a future lesson.\n","\n","**5.1.5. Fill in the code below and run the cell to verify that the alternative closed-form solution for ridge regression gives us the same result:**"]},{"cell_type":"code","metadata":{"id":"dclYX9mcqiit","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607645307524,"user_tz":480,"elapsed":467,"user":{"displayName":"Brian Zhu","photoUrl":"","userId":"06190686625901030588"}},"outputId":"abc11561-f2c9-4655-f43a-a10aeac95ada"},"source":["# Generating the polynomial toy model data again\n","x_range = np.linspace(-3, 1, 101, endpoint=True)\n","func = lambda x: x**3 + 3*x**2 - 2\n","np.random.seed(123)\n","x, y = generate_data(x_range, func, 0.4, 80)\n","\n","N = 40\n","D = 7\n","x_train = x[:N] \n","y_train = y[:N]\n","x_test = x[N:]\n","y_test = y[N:]\n","X_train = get_features(D, x_train)\n","\n","def ridge_alternative(X, y, lambd=0.1):\n","    ### BEGIN CODE ###\n","    w = X.T @ np.linalg.inv(X @ X.T + lambd * np.eye(X.shape[0])) @ y\n","    ### END CODE ###\n","    return w\n","\n","lambd = 0.1\n","w_ridge = ridge(X_train, y_train, lambd)\n","w_ridge_alternative = ridge_alternative(X_train, y_train, lambd)\n","print(f'w_ridge: {w_ridge}')\n","print(f'w_alternative: {w_ridge_alternative}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["w_ridge: [-1.58959136 -0.27968138  2.47927572  1.14850092 -0.24080509 -0.4640939\n"," -0.18200006 -0.02130331]\n","w_alternative: [-1.58959136 -0.27968138  2.47927572  1.14850092 -0.24080509 -0.4640939\n"," -0.18200006 -0.02130331]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0ACtrnQzvVFR"},"source":["## 5.2 Fake Data and Fake Features Perspective"]},{"cell_type":"markdown","metadata":{"id":"MUg9P_lkvZE8"},"source":["We are going to introduce two final perspectives on Ridge Regression, which are the fake data and fake features perspectives. More specifically, we will see that the fake data perspective will net us the standard closed-form solution for ridge regression while the fake features perspective will net us the alternative solution to ridge regression."]},{"cell_type":"markdown","metadata":{"id":"8FKiqrs0vdMz"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Au22zrHMw_Kp"},"source":["**5.2.1. Fake Data Perspective**\n","\n","Given that we have a properly constructed $X$ matrix and $\\vec{y}$ vector, let us add fake data points to $X$ and $\\vec{y}$ such that:\n","\n","$$\\hat{X} = \\begin{bmatrix}\n","X\\\\\n","\\sqrt{\\lambda}I\n","\\end{bmatrix}$$\n","\n","$$\\hat{y}=\\begin{bmatrix}\n","\\vec{y}\\\\\n","0\n","\\end{bmatrix}$$\n","\n","**Show that the closed-form OLS solution using the augmented $\\hat{X}$ matrix and $\\hat{y}$ vector will evaluate to the closed-form solution for ridge regression.**"]},{"cell_type":"markdown","metadata":{"id":"8OfvXhu4vmbY"},"source":["YOUR ANSWER HERE:\r\n","$$\\hat{w}=(\\hat{X}^T\\hat{X})\\hat{X}^T\\hat{y}$$\r\n","$$\\hat{w}=(\\begin{bmatrix}\r\n","X^T  \\sqrt{\\lambda}I\r\n","\\end{bmatrix}\\begin{bmatrix}\r\n","X\\\\\r\n","\\sqrt{\\lambda}I\r\n","\\end{bmatrix})^{-1}\\begin{bmatrix}\r\n","X^T  \\sqrt{\\lambda}I\r\n","\\end{bmatrix}\\begin{bmatrix}\r\n","\\vec{y}\\\\\r\n","0\r\n","\\end{bmatrix}$$\r\n","\r\n","$$\\hat{w}=(X^TX+\\lambda I)^{-1}X^Ty$$"]},{"cell_type":"markdown","metadata":{"id":"Ez9oXpTIvntx"},"source":["**5.2.2. Fill in the code below and run the cell to see that the fake data perspective gives us the same result as ridge**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tUiLtLv2IpfU","executionInfo":{"status":"ok","timestamp":1607645307705,"user_tz":480,"elapsed":640,"user":{"displayName":"Brian Zhu","photoUrl":"","userId":"06190686625901030588"}},"outputId":"ee9c9cf2-000f-4338-b8d0-a6a0c774070a"},"source":["def ridge_fake_data(X, y, lambd = 0.1):\n","    ### BEGIN CODE ###\n","    X_hat = np.vstack((X, np.sqrt(lambd)*np.eye(X.shape[1])))\n","    y_hat = np.hstack((y, np.zeros((X.shape[1]))))\n","    ### END CODE ###\n","    w = ols(X_hat, y_hat)\n","    return w;\n","\n","X_train = get_features(D, x_train)\n","lambd = 0.1\n","w_ridge = ridge(X_train, y_train, lambd)\n","w_fake_data = ridge_fake_data(X_train, y_train, lambd)\n","print(f'w_ridge: {w_ridge}')\n","print(f'w_fake_data: {w_fake_data}')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["w_ridge: [-1.58959136 -0.27968138  2.47927572  1.14850092 -0.24080509 -0.4640939\n"," -0.18200006 -0.02130331]\n","w_fake_data: [-1.58959136 -0.27968138  2.47927572  1.14850092 -0.24080509 -0.4640939\n"," -0.18200006 -0.02130331]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RvruXwCKvvda"},"source":["**5.2.3. Now that we have a learned $\\hat{w}$, explain how we could make predictions on test data $X_{test}$**"]},{"cell_type":"markdown","metadata":{"id":"92Zx2EDgvy9K"},"source":["YOUR ANSWER HERE: We can make predictions $y_{pred}=X_{test}\\hat{w}$"]},{"cell_type":"markdown","metadata":{"id":"ae4O2XGWv-a4"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"k9ZPZBiVGmlz"},"source":["**5.2.4. Fake Features Perspective**\n","Let's augment the data matrix again, except this time we are adding fake features such that:\n","\n","$$\\hat{X} = \\begin{bmatrix}\n","X \\sqrt{\\lambda}I\n","\\end{bmatrix}$$\n","\n","Notice that the $\\hat{X}$ matrix is wide now, so we need to use the minimum-norm solution instead of OLS. In addition the weight vector we find using the minimum-norm solution will actually have two components: one for the original features and one for the fake features. We will show this decomposition by defining the weight vector from the minimum-norm solution as $\\begin{bmatrix}\n","\\hat{w}\\\\\n","\\hat{\\epsilon}\n","\\end{bmatrix}$. Show that the minimum-norm solution with the augmented $\\hat{X}$ matrix will net us the same $\\hat{w}$ as the alternative closed-form solution for ridge regression:"]},{"cell_type":"markdown","metadata":{"id":"0j6XsoarwdM6"},"source":["YOUR ANSWER HERE: \r\n","\r\n","$$\\begin{bmatrix}\r\n","\\hat{w}\\\\\r\n","\\hat{\\epsilon}\r\n","\\end{bmatrix}=\\hat{X}^T(\\hat{X}\\hat{X}^T)^{-1}\\vec{y}$$\r\n","\r\n","$$\\begin{bmatrix}\r\n","\\hat{w}\\\\\r\n","\\hat{\\epsilon}\r\n","\\end{bmatrix}=\\begin{bmatrix}\r\n","X^T \\\\\r\n","\\sqrt{\\lambda}I\r\n","\\end{bmatrix}(\\begin{bmatrix}\r\n","X \\sqrt{\\lambda}I\r\n","\\end{bmatrix}\\begin{bmatrix}\r\n","X^T \\\\\r\n","\\sqrt{\\lambda}I\r\n","\\end{bmatrix})^{-1}\\vec{y}$$\r\n","\r\n","$$\\begin{bmatrix}\r\n","\\hat{w}\\\\\r\n","\\hat{\\epsilon}\r\n","\\end{bmatrix}=\\begin{bmatrix}\r\n","X^T \\\\\r\n","\\sqrt{\\lambda} I\r\n","\\end{bmatrix}(XX^T + \\lambda I)^{-1}\\vec{y}$$\r\n","\r\n","$$\\hat{w}=X^T(XX^T + \\lambda I)^{-1}\\vec{y}$$"]},{"cell_type":"markdown","metadata":{"id":"Wf3-bAltwZq6"},"source":["**5.2.5. Fill in the code below and run the cell to see that the fake features perspective gives us the same result as ridge**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51kAfwwYdMjf","executionInfo":{"status":"ok","timestamp":1607645307705,"user_tz":480,"elapsed":634,"user":{"displayName":"Brian Zhu","photoUrl":"","userId":"06190686625901030588"}},"outputId":"f8248109-65bd-4071-ee3e-3fe7f5af3034"},"source":["def ridge_fake_features(X, y, lambd = 0.1):\n","    X_hat = np.hstack((X, np.sqrt(lambd)*np.eye(X.shape[0])))\n","    ### BEGIN CODE ###\n","    we = X_hat.T@np.linalg.inv(X_hat@X_hat.T)@y\n","    w = we[:X.shape[1]]\n","    ### END CODE ###\n","    return w;\n","\n","X_train = get_features(D, x_train)\n","lambd = 1\n","w_ridge_alternative = ridge_alternative(X_train, y_train, lambd)\n","w_fake_data = ridge_fake_features(X_train, y_train, lambd)\n","print(f'w_ridge_alternative: {w_ridge_alternative}')\n","print(f'w_fake_data: {w_fake_data}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["w_ridge_alternative: [-1.18373602 -0.19662124  0.91983449 -0.0317246   0.14772396  0.14705711\n","  0.013015   -0.00160606]\n","w_fake_data: [-1.18373602 -0.19662124  0.91983449 -0.0317246   0.14772396  0.14705711\n","  0.013015   -0.00160606]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K36eXlCAqu2V"},"source":["**5.2.6. Now imagine instead of only keeping $\\hat{w}$, we actually kept the entire $(n\\times d)$-dimension weight vector from the minimum norm solution: $\\begin{bmatrix}\n","\\hat{w}\\\\\n","\\hat{\\epsilon}\n","\\end{bmatrix}$. Explain how we could augment $X_{test}$ so our predictions are equivalent to $X_{test}\\hat{w}$.**"]},{"cell_type":"markdown","metadata":{"id":"CyoIduq0woPY"},"source":["YOUR ANSWER HERE: We can augment $X_{test}$ such that: $\\hat{X}_{test}=\\begin{bmatrix}\r\n","X_{test} 0_{n\\times n}\r\n","\\end{bmatrix}$ and make predictions:\r\n","$$y_{pred}=\\hat{X}_{test}\\begin{bmatrix}\r\n","\\hat{w}\\\\\r\n","\\hat{\\epsilon}\r\n","\\end{bmatrix} = \\begin{bmatrix}\r\n","X_{test} 0_{n\\times n}\r\n","\\end{bmatrix}\\begin{bmatrix}\r\n","\\hat{w}\\\\\r\n","\\hat{\\epsilon}\r\n","\\end{bmatrix} = X_{test}\\hat{w}$$"]}]}